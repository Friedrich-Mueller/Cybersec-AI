{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Dataset is not just directly downloadable. A variety of scripts are provided to download and organize the dataset.\n",
    "here: https://github.com/gsantia/BuzzFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessecary IDs:\n",
    "\n",
    "\n",
    "Facebook App-ID:\n",
    "\n",
    "495004908666220\n",
    "\n",
    "Facebook App-Secret-ID:\n",
    "\n",
    "72b126cb77822118f4fd073882c191ac\n",
    "\n",
    "\n",
    "Disqus App-ID:\n",
    "\n",
    "VofvRAkalgJD7PZ6aXeGgmC0namGHWKoc7wSQULBEspKmvffoIcp5loTIXpDBN5N\n",
    "\n",
    "Disqus App-Secret-ID:\n",
    "\n",
    "GnDDAudnA36Lyr4hCpJRKxnM85oTjudWWWbLPZF2R5sIMkIKiIt5FknL3NJHBtu0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape.py\n",
    "Access the news articles themselves and download the relevant data.\n",
    "Also collect the Facebook Plugin and Disqus Plugin comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import glob, json, os, re, time\n",
    "import urllib.request as urllib2\n",
    "import http.client as httplib\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "APP_ID = \"495004908666220\"\n",
    "APP_SECRET = \"72b126cb77822118f4fd073882c191ac\"\n",
    "\n",
    "DISQUS_ID = \"VofvRAkalgJD7PZ6aXeGgmC0namGHWKoc7wSQULBEspKmvffoIcp5loTIXpDBN5N\"\n",
    "DISQUS_SECRET = \"GnDDAudnA36Lyr4hCpJRKxnM85oTjudWWWbLPZF2R5sIMkIKiIt5FknL3NJHBtu0\"\n",
    "\n",
    "def scrapeAddicting(webpage):\n",
    "    \"\"\"scrapes important info from an Addicting Info _webpage_\"\"\"\n",
    "\n",
    "    articleJSON, page = createPage(webpage)\n",
    "    soup = BeautifulSoup(page)\n",
    "\n",
    "    #the body text is all contained in <p> tags\n",
    "    raw_text = soup.body.find_all('p')\n",
    "    text = []\n",
    "    for para in raw_text:\n",
    "        if para.get_text():\n",
    "            if not para.find('em'): #not part of the body text\n",
    "                text.append(para.get_text())\n",
    "                links = para.find_all('a')\n",
    "                if links:\n",
    "                    for link in links:\n",
    "                        link_url = link['href']\n",
    "                        link_text = link.get_text()\n",
    "                        articleJSON['links'].append([link_url, link_text])\n",
    "                        if 'https://twitter.com' in link_url:\n",
    "                            if 'status' in link_url:\n",
    "                                articleJSON['tweets'].append(link_url)\n",
    "        #now get the pictures\n",
    "        pics = para.find_all('img')\n",
    "        if pics:\n",
    "            for pic in pics:\n",
    "                pic_url = pic['src']\n",
    "                pic_text = pic.get('alt', '')\n",
    "                articleJSON['pictures'].append([pic_url, pic_text])\n",
    "\n",
    "    articleJSON['body'] = \" \".join(text)\n",
    "    #no Disqus or FB comments\n",
    "    \"\"\"\n",
    "    There is no robots.txt for the Addicting Info site, and no mention\n",
    "    of rate limiting. In this case, wait 30 seconds between scrapes,\n",
    "    just to be safe.\n",
    "    \"\"\"\n",
    "    time.sleep(30)\n",
    "    return articleJSON\n",
    "\n",
    "def scrapePolitico(webpage):\n",
    "    \"\"\"takes the url of a politi.co article and scrapes important info\n",
    "    using BeautifulSoup\"\"\"\n",
    "    #pictures key will have a list of tuples of image link and its caption\n",
    "    #links key will be list of tuples of link and its naming\n",
    "    articleJSON, page = createPage(webpage)\n",
    "    soup = BeautifulSoup(page, 'html5lib')\n",
    "\n",
    "    #first find the figures and their captions\n",
    "    figures = soup.find_all('figure', class_ = 'art')\n",
    "    if figures:\n",
    "        for figure in figures:\n",
    "            captions = figure.find_all('p', class_ = '')\n",
    "            if captions:\n",
    "                caption = captions[0].get_text().strip()\n",
    "            else:\n",
    "                caption = \"\"\n",
    "            images = figure.find_all('img')\n",
    "            if images:\n",
    "                if images[0]['src']:\n",
    "                    image = images[0]['src']\n",
    "                else:\n",
    "                    image = \"\"\n",
    "            else:\n",
    "                image = \"\"\n",
    "            articleJSON[\"pictures\"].append((image, caption))\n",
    "\n",
    "    #now find the links and their text\n",
    "    links = [x\n",
    "             for x in soup.body.find_all('a')\n",
    "             if x.parent.name == 'p']\n",
    "    #the last one is just an ad\n",
    "    if links:\n",
    "        for link in links:\n",
    "            #get rid of the ad at the bottom for some of the articles\n",
    "            if not link.parent.get('class'):\n",
    "                link.parent['class'] = ''\n",
    "            if link.get_text() != 'POLITICO Playbook':\n",
    "                if link.parent['class'] != 'category':\n",
    "                    url = link['href'] if link['href'] else \"\"\n",
    "                    text = link.get_text().strip()\n",
    "                    articleJSON['links'].append((url, text))\n",
    "    paras = [x\n",
    "             for x in soup.body.find_all('p', class_ = \"\")\n",
    "             if x.parent.name != \"figcaption\"]\n",
    "    #these are the <p> tags with the body text, minus those for figs\n",
    "    #and the blurbs at the bottom\n",
    "    if paras:\n",
    "        raw_text = [x.get_text().strip() for x in paras]\n",
    "        #seems like the last <p> is just going to be an ad, delete it\n",
    "        article = \" \".join(raw_text[:-1])\n",
    "        articleJSON[\"body\"] = article\n",
    "    #get tweets\n",
    "    articleJSON['tweets'] = getTweets(soup)\n",
    "    #just FB comments\n",
    "    \"\"\"\n",
    "    The robots.txt file and Terms of Service for the Politico site\n",
    "    makes no mention of rate limiting. So let's just go with the \n",
    "    standard.\n",
    "    \"\"\"\n",
    "    time.sleep(30)\n",
    "    return articleJSON\n",
    "\n",
    "def scrapeRightWing(webpage):\n",
    "    \"\"\"scrapes important info from RightWingNews articles\"\"\"\n",
    "    articleJSON, page = createPage(webpage)\n",
    "    soup = BeautifulSoup(page, 'html5lib')\n",
    "    raw_text = soup.body.find_all('p')\n",
    "    #get the body text and the links\n",
    "    text = []\n",
    "    for para in raw_text:\n",
    "        if 'main-article' in para.parent.get('class', []):  #just want content\n",
    "            if para.get_text():\n",
    "                text.append(para.get_text().strip())\n",
    "                #get the links here too\n",
    "                links = para.find_all('a')\n",
    "                if links:\n",
    "                    for link in links:\n",
    "                        link_url = link['href']\n",
    "                        link_text = link.get_text()\n",
    "                        articleJSON['links'].append([link_url, link_text])\n",
    "    body = \" \".join(text)\n",
    "    articleJSON['body'] = body\n",
    "    #now scrape the pictures\n",
    "    centered = soup.body.find_all('center')\n",
    "    photos = soup.find_all('img', class_ = 'article-photo')\n",
    "    for photo in photos:\n",
    "        photo_text = photo.get_text().strip()\n",
    "        photo_url = photo['src']\n",
    "        articleJSON['pictures'].append([photo_text, photo_url])\n",
    "    for center in centered:\n",
    "        images = center.find_all('img')\n",
    "        for image in images:\n",
    "            image_text = image.get_text().strip()\n",
    "            image_url = image['src']\n",
    "            articleJSON['pictures'].append([image_url, image_text])\n",
    "    articleJSON['tweets'] = getTweets(soup)\n",
    "    #just FB comments\n",
    "    \"\"\"\n",
    "    The robots.txt file for Right Wing News says there must be a\n",
    "    delay of 30 seconds between scrapes, so that's what we'll do.\n",
    "    There is no mention of restrictions in the Terms of Service.\n",
    "    \"\"\"\n",
    "    time.sleep(30)\n",
    "    return articleJSON\n",
    "\n",
    "def scrapeEagle(webpage):\n",
    "    \"\"\"scrapes important info from Eagle Rising articles\"\"\"\n",
    "    articleJSON, page = createPage(webpage)\n",
    "    soup = BeautifulSoup(page)\n",
    "    text = []\n",
    "    raw_text = soup.article.find_all('p')\n",
    "    for para in raw_text:\n",
    "        if para.get_text():\n",
    "            text.append(para.get_text().strip())\n",
    "            #get the links\n",
    "            links = para.find_all('a')\n",
    "            if links:\n",
    "                for link in links:\n",
    "                    link_url = link['href']\n",
    "                    link_text = link.get_text().strip()\n",
    "                    articleJSON['links'].append([link_url, link_text])\n",
    "        #get the images\n",
    "        images = para.find_all('img')\n",
    "        if images:\n",
    "            for image in images:\n",
    "                image_url = image.get('src')\n",
    "                image_caption = image.get('alt').strip()\n",
    "                articleJSON['pictures'].append([image_url, image_caption])\n",
    "    articleJSON['body'] = \" \".join(text)\n",
    "    articleJSON['tweets'] = getTweets(soup)\n",
    "    #has both FB and Disqus comments!\n",
    "    articleJSON['DisqComm'] = getDisqComments(webpage, forum = 'eaglerising')\n",
    "    \"\"\"\n",
    "    The robots.txt file for Eagle Rising designates the delay time to be\n",
    "    10 seconds, so that's what we'll use. The Terms of Service make no mention\n",
    "    of it.\n",
    "    \"\"\"\n",
    "    time.sleep(10)\n",
    "    return articleJSON\n",
    "\n",
    "def scrapeOccupy(webpage):\n",
    "    \"\"\"scrapes important info from OccupyDemocrats articles\"\"\"\n",
    "    articleJSON, page = createPage(webpage)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "    #get the main article image\n",
    "    main_pic = soup.find_all('img', class_ = \"attachment- size- wp-post-image\")\n",
    "    if main_pic:\n",
    "        main_url = main_pic[0]['src']\n",
    "        main_caption = main_pic[0].get_text().strip()\n",
    "        articleJSON['pictures'].append([main_url, main_caption])\n",
    "    other_pics = soup.body.find_all('img')\n",
    "    other_pics = [x\n",
    "                  for x in other_pics\n",
    "                  if x.parent.name == 'p']\n",
    "    if other_pics:\n",
    "        for other_pic in other_pics:\n",
    "            other_cap = other_pic.get_text()\n",
    "            other_url = other_pic['src']\n",
    "            articleJSON['pictures'].append([other_url, other_cap])\n",
    "\n",
    "    articles = soup.body.find_all('article')\n",
    "    raw_text = [x\n",
    "                for x in articles[0].find_all('p', class_ = '', lang='', dir='')\n",
    "                if x.get_text() != ''][1:-1]\n",
    "    #this removes the author blurbs and blank <p>s\n",
    "    #get the body links and pics\n",
    "    text = []\n",
    "    for para in raw_text:\n",
    "        text.append(para.get_text().strip())\n",
    "        #links\n",
    "        if para.find_all('a'):\n",
    "            for link in para.find_all('a'):\n",
    "                url = link['href']\n",
    "                link_text = link.get_text().strip()\n",
    "                articleJSON['links'].append([url, link_text])\n",
    "        #pictures\n",
    "        if para.find_all('img'):\n",
    "            for pic in para.find_all('img'):\n",
    "                pic_url = pic['src']\n",
    "                pic_caption = pic.get_text().strip()\n",
    "                articleJSON['pictures'].append([pic_url, pic_caption])\n",
    "\n",
    "    articleJSON['body'] = \" \".join(text)\n",
    "    articleJSON['tweets'] = getTweets(soup)\n",
    "    #just FB comments\n",
    "    \"\"\"\n",
    "    The Occupy Democrats robot.txt makes no mention of rate-limiting,\n",
    "    so we'll just use the default.\n",
    "    \"\"\"\n",
    "    time.sleep(30)\n",
    "    return articleJSON\n",
    "\n",
    "def scrapeCNN(webpage):\n",
    "    \"\"\"scrapes important info from CNN articles\"\"\"\n",
    "    articleJSON, page = createPage(webpage)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "    raw_text = soup.body.find_all(class_ = 'zn-body__paragraph')\n",
    "    pictures = []\n",
    "    metas = soup.find_all('meta')\n",
    "    for meta in metas:\n",
    "        if meta:\n",
    "            if 'content' in meta.attrs.keys():\n",
    "                try:\n",
    "                    if (meta['content'][:7] == 'http://'\n",
    "                            and meta['content'][-4:] == '.jpg'):\n",
    "                        pictures.append(meta['content'])\n",
    "                except:\n",
    "                    print(\"bad url!\")\n",
    "    #AFAIK cnn doesn't have captions on its vids\n",
    "    articleJSON['pictures'] = [(pic, '') for pic in set(pictures)]\n",
    "    #now get the links\n",
    "    links = []\n",
    "    text = []\n",
    "    for para in raw_text:\n",
    "        if para.get_text():\n",
    "            text.append(para.get_text().strip())\n",
    "        if para.find_all('a'):\n",
    "            for link in para.find_all('a'):\n",
    "                url = link['href']\n",
    "                link_text = link.get_text().strip()\n",
    "                links.append([url, link_text])\n",
    "    #get the tweets, just the URLS\n",
    "    articleJSON['tweets'] = getTweets(soup)\n",
    "    articleJSON['body'] = \" \".join(text)\n",
    "    articleJSON['links'] = links\n",
    "    #no comments\n",
    "    \"\"\"\n",
    "    The robots.txt for CNN makes no mention of rate-limiting, so \n",
    "    let's just use the default value. The Terms of Service make no\n",
    "    mention of it.\n",
    "    \"\"\"\n",
    "    time.sleep(30)\n",
    "    return articleJSON\n",
    "\n",
    "def scrapeFreedom(webpage):\n",
    "    \"\"\"scrapes articles on Freedom Daily website\"\"\"\n",
    "    articleJSON, page = createPage(webpage)\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    text = []\n",
    "    raw_text = soup.body.article.find_all('p')\n",
    "    if raw_text:\n",
    "        for para in soup.body.article.find_all('p'):\n",
    "            #body text\n",
    "            if para.get_text().strip():\n",
    "                text.append(para.get_text().strip())\n",
    "            #links\n",
    "            if para.find_all('a'):\n",
    "                for link in para.find_all('a'):\n",
    "                    url = link['href']\n",
    "                    link_text = link.get_text().strip()\n",
    "                    articleJSON['links'].append([url, link_text])\n",
    "    #get images\n",
    "    images = soup.body.article.find_all('img')\n",
    "    if images:\n",
    "        for image in soup.body.article.find_all('img'):\n",
    "            image_url = image.get('src', '')\n",
    "            image_caption = image.get('title', '')\n",
    "            articleJSON['pictures'].append([image_url, image_caption])\n",
    "\n",
    "    articleJSON['tweets'] = getTweets(soup)\n",
    "    articleJSON['body'] = \" \".join(text)\n",
    "    #just FB comments\n",
    "    \"\"\"\n",
    "    The robots.txt for Freedom Daily makes no mention of\n",
    "    rate-limiting, so let's just use the default.\n",
    "    \"\"\"\n",
    "    time.sleep(30)\n",
    "    return articleJSON\n",
    "\n",
    "def scrape98(webpage):\n",
    "    \"\"\"scrapes The Other 98% pages\"\"\"\n",
    "    #there are 12 Occupy Democrat articles in here, so we can just use the\n",
    "    #function we already made. then there are around 75 facebook links,\n",
    "    #which are just photos or videos. the actual articles are mainly\n",
    "    #usuncut.com, so should make a scraper for that.\n",
    "\n",
    "    #first take care of Occupy articles\n",
    "    if 'http://occupydemocrats.com' in webpage:\n",
    "        return scrapeOccupy(webpage)\n",
    "    elif 'http://usuncut.com' in webpage:\n",
    "        return scrapeUncut(webpage)\n",
    "    else:   #facebook links, other stuff.\n",
    "        return {}\n",
    "\n",
    "def scrapeUncut(webpage):\n",
    "    \"\"\"scrapes the US Uncut links found in the 98% articles\"\"\"\n",
    "    # this site is down??\n",
    "    return {}\n",
    "\n",
    "def scrapeABC(webpage):\n",
    "    \"\"\"scrape ABC News articles\"\"\"\n",
    "    articleJSON, page = createPage(webpage)\n",
    "    text = []\n",
    "    if page:\n",
    "        #check for redirect here\n",
    "        if page.geturl() != \"http://abcnews.go.com\":\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "            raw_text = soup.body.find_all('p', itemprop = 'articleBody')\n",
    "            for para in raw_text:\n",
    "                if para.get_text().strip():\n",
    "                    text.append(para.get_text().strip())\n",
    "                #get links\n",
    "                links = para.find_all('a')\n",
    "                if links:\n",
    "                    for link in links:\n",
    "                        link_url = link.get('href', '')\n",
    "                        link_text = link.get_text()\n",
    "                        articleJSON['links'].append([link_url, link_text])\n",
    "        pics = soup.body.find_all('picture')\n",
    "        for pic in pics:\n",
    "            picture = pic.find('img')\n",
    "            articleJSON['pictures'].append([picture.get('src', '').strip(), picture.get('alt', '').strip()])\n",
    "        articleJSON['tweets'] = getTweets(soup)\n",
    "        articleJSON['body'] = \" \".join(text)\n",
    "        #just Disqus comments\n",
    "        articleJSON['DisqComm'] = getDisqComments(webpage, 'abcnewsdotcom')\n",
    "    \"\"\"\n",
    "    The robots.txt makes no mention of rate-limiting, so we'll use the default.\n",
    "    Terms of Service makes no mention either.\n",
    "    \"\"\"\n",
    "    time.sleep(30)\n",
    "    return articleJSON\n",
    "\n",
    "\n",
    "def getTweets(soup):\n",
    "    \"\"\"scrapes the embedded tweets on pages. they seem to be the same format on\n",
    "    the different websites. takes a soup object in and gives back a list of\n",
    "    tweet urls\"\"\"\n",
    "    article_tweets = []\n",
    "    tweets = soup.body.find_all('blockquote', class_ = 'twitter-tweet')\n",
    "    if tweets:\n",
    "        for tweet in tweets:\n",
    "            tweet_urls = tweet.find_all('a')\n",
    "            temp = [tweet['href'] for tweet in tweet_urls]\n",
    "            real_urls = filter(lambda x: True if 'status' in x else False,\n",
    "                                  temp)\n",
    "            article_tweets.extend(real_urls)\n",
    "\n",
    "    return article_tweets\n",
    "\n",
    "def createPage(webpage):\n",
    "    \"\"\"given a _webpage_ will set up the right environment and then opens and\n",
    "    returns the page using urllib2\"\"\"\n",
    "    articleJSON = {\n",
    "                   \"body\" : \"\",\n",
    "                   \"pictures\" : [],\n",
    "                   \"links\" : [],\n",
    "                   \"tweets\" : [],\n",
    "                   \"comments\" : [],\n",
    "                   \"DisqComm\" : []  #keep the Disqus comments separate\n",
    "                  }\n",
    "    page = openPage(webpage)\n",
    "    return articleJSON, page\n",
    "\n",
    "def openPage(webpage):\n",
    "    \"\"\"given a webpage, opens the page in Python and returns it\"\"\"\n",
    "    #to avoid 403 errors, need to make the request as a browser\n",
    "    web_response = None\n",
    "    hdr = {'User-agent' : 'Mozilla/5.0'}\n",
    "    req = urllib2.Request(webpage, headers=hdr)\n",
    "    #open the page, try to catch all exceptions\n",
    "    try:\n",
    "        web_response = urllib2.urlopen(req)\n",
    "    except urllib2.HTTPError as e:\n",
    "        print(e.fp.read())\n",
    "    except urllib2.URLError as e:\n",
    "        #print(e.fp.read())\n",
    "        print(\"URL ERROR \", wepage)\n",
    "    except httplib.HTTPException as e:\n",
    "        #print(e.fp.read())\n",
    "        print(\"HTTP EXCEPTION \", webpage)\n",
    "    except Exception as e:\n",
    "        print(\"other exception loading the page: \", webpage)\n",
    "\n",
    "    return web_response\n",
    "\n",
    "def createCommentsURL(webpage, APP_ID, APP_SECRET):\n",
    "    \"\"\"some of the articles have embedded facebook comments. this will create\n",
    "    the correct URL to grab these comments in JSON form\"\"\"\n",
    "    #to scrape fb comments on other urls first need to find the ID number\n",
    "    #assigned to the page by fb\n",
    "    ID_URL = 'https://graph.facebook.com/?ids='\n",
    "    url = ID_URL + webpage\n",
    "    web_response = openPage(url)\n",
    "    if web_response:\n",
    "        readable_page = web_response.read()\n",
    "    else:\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(readable_page)\n",
    "    except:\n",
    "        print(\"problem loading comment json\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        #the graph API requires an ID number to access comments\n",
    "        ID_NUM = data[webpage]['og_object']['id']\n",
    "\n",
    "        comments_args = \"/comments?access_token=\" + APP_ID + \"|\" + APP_SECRET + \\\n",
    "                            \"&filter=stream\"\n",
    "        comments_url = (\"https://graph.facebook.com/v2.10/\" +\n",
    "                            ID_NUM + comments_args)\n",
    "    except:\n",
    "        print(\"problem assigning fb comments URL\")\n",
    "        comments_url = \"\"\n",
    "    return comments_url\n",
    "\n",
    "def getComments(webpage, n = 1):\n",
    "    \"\"\"fetches the embedded facebook plugin comments for the article at\n",
    "    _webpage_. returns the data in a JSON. can't use the collectPosts version\n",
    "    because that only works for facebook posts.\"\"\"\n",
    "    comments_url = createCommentsURL(webpage, APP_ID, APP_SECRET)\n",
    "    data = []\n",
    "\n",
    "    if comments_url:\n",
    "        time.sleep(n)\n",
    "        web_response = openPage(comments_url)\n",
    "        if web_response:\n",
    "            readable_page = web_response.read()\n",
    "        else:\n",
    "            return {}\n",
    "        try:\n",
    "            thisdata = json.loads(readable_page)\n",
    "            data.extend(thisdata['data'])\n",
    "        except:\n",
    "            print(\"loading comment didn't work\")\n",
    "\n",
    "        if 'paging' in thisdata.keys():\n",
    "            while thisdata['paging'].get('next', False):\n",
    "                try:\n",
    "                    time.sleep(n)\n",
    "                    web_response = urllib2.urlopen(thisdata['paging']['next'])\n",
    "                    if web_response:\n",
    "                        thisdata = json.loads(web_response.read())\n",
    "                    if thisdata['data']:\n",
    "                        data.extend(thisdata['data'])\n",
    "                except:\n",
    "                    print(\"problem getting extra batches\")\n",
    "    return data\n",
    "\n",
    "def createDisqURL(webpage, DISQUS_ID, DISQUS_SECRET, forum):\n",
    "    \"\"\"some of the articles have embedded disqus comments. this will create the\n",
    "    correct URL to make API requests\"\"\"\n",
    "    try:\n",
    "        if forum == 'abcnewsdotcom':\n",
    "            _, page = createPage(webpage)   #take care of redirects\n",
    "            url = page.geturl()\n",
    "            page = openPage(url)\n",
    "            soup = BeautifulSoup(page)\n",
    "            ID = soup.body.find('article')['data-id']   #get the ID of the article\n",
    "\n",
    "            split = re.split('/', url[7:])   #need to reformat the link\n",
    "            webpage = 'http://' + split[0] + '/' + split[1] + '/story?id=' + ID\n",
    "\n",
    "\n",
    "        args = '&thread=link:' + webpage + '&forum=' + forum\n",
    "        comments_url = ('https://disqus.com/api/3.0/threads/listPosts.json?api_key=' +\n",
    "                            DISQUS_ID + '&api_secret=' + DISQUS_SECRET + args)\n",
    "    except:\n",
    "        print(\"problem creating Disqus URL\")\n",
    "        comments_url = \"\"\n",
    "    return comments_url\n",
    "\n",
    "def getDisqComments(webpage, forum, n = 1):\n",
    "    \"\"\"makes the requests to get the Disqus comments for a given _webpage_,\n",
    "    also requires the _forum_ name for the page as per the API\"\"\"\n",
    "    comments_url = createDisqURL(webpage, DISQUS_ID, DISQUS_SECRET, forum)\n",
    "    data = []\n",
    "    if comments_url:\n",
    "        time.sleep(n)\n",
    "        web_response = openPage(comments_url)\n",
    "        if web_response:\n",
    "            readable_page = web_response.read()\n",
    "        else:\n",
    "            return {}\n",
    "        try:\n",
    "            thisdata = json.loads(readable_page)\n",
    "            data.extend(thisdata['response'])\n",
    "        except:\n",
    "            print(\"loading comments didn't work\")\n",
    "            return {}\n",
    "\n",
    "        if 'cursor' in thisdata.keys():\n",
    "            while thisdata['cursor']['hasNext']:\n",
    "                \"\"\"\n",
    "                I'm not 100% sure what the rate limit is for the Disqus API,\n",
    "                can't find it in the documentation. To be safe, just use the\n",
    "                same rate we've been using for the facebook comments.\n",
    "                \"\"\"\n",
    "                time.sleep(n)\n",
    "                next_cursor = thisdata['cursor']['next']\n",
    "                extra_comments_url = comments_url + '&cursor=' + next_cursor\n",
    "                try:\n",
    "                    web_response = openPage(extra_comments_url)\n",
    "                except:\n",
    "                    print(\"web response\")\n",
    "                    continue\n",
    "                try:\n",
    "                    readable_page = web_response.read()\n",
    "                except:\n",
    "                    print(\"readable page\")\n",
    "                    continue\n",
    "                try:\n",
    "                    thisdata = json.loads(readable_page)\n",
    "                    if thisdata.get('response', False):\n",
    "                        data.extend(thisdata['response'])\n",
    "                except:\n",
    "                    print(\"json load?\")\n",
    "                    continue\n",
    "\n",
    "    return data\n",
    "\n",
    "def outlet_scrape(outlet):\n",
    "    \"\"\"given an _outlet_ will scrape all articles contained in that folder\"\"\"\n",
    "    outletDict = {\n",
    "                  \"Politico\" : scrapePolitico,\n",
    "                  \"CNN_Politics\" : scrapeCNN,\n",
    "                  \"Right_Wing_News\" : scrapeRightWing,\n",
    "                  \"Occupy_Democrats\" : scrapeOccupy,\n",
    "                  \"Eagle_Rising\" : scrapeEagle,\n",
    "                  \"Addicting_Info\" : scrapeAddicting,\n",
    "                  \"Freedom_Daily\" : scrapeFreedom,\n",
    "                  \"The_Other_98%\" : scrape98,\n",
    "                  \"ABC_News_Politics\" : scrapeABC\n",
    "                  }\n",
    "    path = './dataset/' + outlet + '/'\n",
    "\n",
    "    files = glob.glob(path + '*/posts.json')\n",
    "    num = len(files)\n",
    "    for i, filename in enumerate(files):\n",
    "        print(\"working on: \" + filename)\n",
    "        print(str(i + 1) + \" / \" + str(num))\n",
    "        fileJSON = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            postJson = json.load(f)\n",
    "\n",
    "        if \"link\" not in postJson.keys():\n",
    "            print(\"no link!\", filename)\n",
    "            continue    #if the link is missing, skip\n",
    "        if \"type\" not in postJson.keys():\n",
    "            print(\"no type! \", filename)\n",
    "        #for now just skip video and pic posts\n",
    "        if postJson.get('type') == 'video':\n",
    "            fileJSON = {\n",
    "                        \"body\" : postJson.get('message', ''),\n",
    "                        \"links\" : [postJson.get('source', ''), ''],\n",
    "                        \"pictures\" : [postJson.get('picture', ''), postJson.get('name', '')]\n",
    "                       }\n",
    "        elif postJson.get('type') == 'photo':\n",
    "            fileJSON = {\n",
    "                        \"body\" : '',\n",
    "                        \"links\" : [postJson.get('link', ''), postJson.get('story', '')],\n",
    "                        \"pictures\" : [postJson.get('picture', ''), postJson.get('name', '')]\n",
    "                       }\n",
    "        else:\n",
    "            #if the post is an article, scrape it\n",
    "            if 'link' not in postJson.keys():\n",
    "                print(\"no link!\")\n",
    "                continue\n",
    "            webpage = postJson[\"link\"]\n",
    "            if outlet == \"The_Other_98%\":\n",
    "                #The Other 98% seems to have NO first party articles, so handle\n",
    "                #this case separately\n",
    "                try:\n",
    "                    fileJSON = scrape98(webpage)\n",
    "                except:\n",
    "                    print(\"scraping didn't work! \", webpage)\n",
    "                fileJSON['comments'] = getComments(webpage)\n",
    "\n",
    "            else:\n",
    "                #these scrapers are only useful for first party articles\n",
    "                if postJson[\"first_party\"]:\n",
    "                    try:\n",
    "                        fileJSON = outletDict[outlet](webpage)\n",
    "                    except:\n",
    "                        print(\"scraping didn't work! \", webpage)\n",
    "                    fileJSON['comments'] = getComments(webpage)\n",
    "\n",
    "\n",
    "        #chop off the posts.json of the filename and add scraped.json\n",
    "        new_filename = filename[:-10] + 'scraped.json'\n",
    "        #now dump the scraped JSONs in the same folder\n",
    "        with open(new_filename, 'w') as f2:\n",
    "            json.dump(fileJSON, f2, indent = 4, sort_keys = True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    outlets = ['ABC_News_Politics',\n",
    "               'Addicting_Info',\n",
    "               'CNN_Politics',\n",
    "               'Eagle_Rising',\n",
    "               'Freedom_Daily',\n",
    "               'Occupy_Democrats',\n",
    "               'Politico',\n",
    "               'Right_Wing_News',\n",
    "                'The_Other_98%']\n",
    "    for outlet in outlets:\n",
    "        if os.path.isdir('./dataset/' + outlet):\n",
    "            outlet_scrape(outlet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collectPosts.py\n",
    "A Python script which goes through all the Facebook posts in the BuzzFeed set and obtains their metadata, comments, and attachments. Dumps this data into posts.json, comments.json, and attach.json files, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:  ['184096565021911', '1035057923259100', 'mainstream', 'ABC News Politics', 'https://www.facebook.com/ABCNewsPolitics/posts/1035057923259100', '2016-09-19', 'video', 'no factual content', '', '', '146', '15']\n",
      "working on 1035057923259100\n",
      "generated post url: https://graph.facebook.com/v3.0/184096565021911_1035057923259100?access_token=495004908666220|72b126cb77822118f4fd073882c191ac&fields=link,message,created_time,name,story,caption,description,picture,place,shares,source,updated_time\n",
      "post error\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'dataset/ABC_News_Politics/1035057923259100'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1ce30f3c3bff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mattachfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutlet\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPOST_ID\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/attach.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;31m#             print(wd + outlet + '/' + POST_ID + '/posts.json')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutlet\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPOST_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;31m#             os.makedirs(commentfile)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;31m#             os.makedirs(attachfile)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cai\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'dataset/ABC_News_Politics/1035057923259100'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import urllib.request as urllib2\n",
    "import json\n",
    "import os, re, csv\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def createFeedUrl(username, APP_ID, APP_SECRET, limit):\n",
    "    post_args = \"/feed?access_token=\" + APP_ID + \"|\" + APP_SECRET + \\\n",
    "                \"&fields=attachments,created_time,message&limit=\" + str(limit)\n",
    "    post_url = \"https://graph.facebook.com/v3.0/\" + username + post_args\n",
    "    return post_url\n",
    "\n",
    "def createPostUrl(USER_ID,POST_ID, APP_ID, APP_SECRET):\n",
    "    post_args = \"?access_token=\" + APP_ID + \"|\" + APP_SECRET + \\\n",
    "                    \"&fields=link,message,created_time,name,story,caption,description,picture,place,shares,source,updated_time\"\n",
    "    post_url = \"https://graph.facebook.com/v3.0/\" + USER_ID + \"_\" + POST_ID + post_args\n",
    "    return post_url\n",
    "\n",
    "def createPostCommentsUrl(USER_ID, POST_ID, APP_ID, APP_SECRET):\n",
    "    comments_args = \"/comments?access_token=\" + APP_ID + \"|\" + APP_SECRET + \\\n",
    "                    \"&order=chronological&limit=1000\"\n",
    "    comments_url = (\"https://graph.facebook.com/v3.0/\" + USER_ID + \"_\" +\n",
    "                        POST_ID + comments_args)\n",
    "    return comments_url\n",
    "\n",
    "def createPostAttachmentsUrl(USER_ID,POST_ID, APP_ID, APP_SECRET):\n",
    "    attachments_args = \"/attachments?access_token=\" + APP_ID + \"|\" + APP_SECRET\n",
    "    attachments_url = (\"https://graph.facebook.com/v3.0/\" + USER_ID + \"_\" +\n",
    "                        POST_ID + attachments_args)\n",
    "    return attachments_url\n",
    "\n",
    "def createPostReactionsUrl(USER_ID,POST_ID, APP_ID, APP_SECRET):\n",
    "    reactions_args = \"/reactions?access_token=\" + APP_ID + \"|\" + APP_SECRET + \\\n",
    "                    \"&limit=1000\"  \n",
    "    reactions_url = (\"https://graph.facebook.com/v3.0/\" + USER_ID + \"_\" + POST_ID +\n",
    "                        reactions_args)\n",
    "    return reactions_url\n",
    "\n",
    "def new_createPostReactionsUrl(USER_ID, POST_ID, APP_ID, APP_SECRET):\n",
    "    reactions_args = \"/reactions?access_token=\" + APP_ID + \"|\" + APP_SECRET + \\\n",
    "            \"&summary=total_count\"\n",
    "    reactions_url = \"https://graph.facebook.com/v3.0/\" + USER_ID + \"_\" + POST_ID + \\\n",
    "            reactions_args\n",
    "    return reactions_url\n",
    "\n",
    "def getUserFeed(username, limit):\n",
    "    post_url = createPostUrl(username, APP_ID, APP_SECRET, limit)\n",
    "    web_response = urllib2.urlopen(post_url)\n",
    "    readable_page = web_response.read()\n",
    "    return json.loads(readable_page)\n",
    "\n",
    "def getPost(USER_ID, POST_ID):\n",
    "    ## have this build urls for the comments, reactions, and attachments\n",
    "    post_url = createPostUrl(USER_ID, POST_ID, APP_ID, APP_SECRET)\n",
    "    print(\"generated post url:\", post_url)\n",
    "    web_response = urllib2.urlopen(post_url)\n",
    "    readable_page = web_response.read()\n",
    "    return json.loads(readable_page)\n",
    "\n",
    "def getPostReactions(USER_ID, POST_ID):\n",
    "    \"\"\"returns a JSON of a post's reactions\"\"\"\n",
    "    ###############################################\n",
    "    # This code is deprecated! Facebook changed the\n",
    "    # API on Feb 5, 2018, so now none of this works.\n",
    "    ###############################################\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    post_url = createPostReactionsUrl(USER_ID, POST_ID, APP_ID, APP_SECRET)\n",
    "    opener = urllib2.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    urllib2.install_opener(opener)\n",
    "    batchnum = 1\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        web_response = urllib2.urlopen(post_url)\n",
    "        readable_page = web_response.read()\n",
    "        thisdata = json.loads(readable_page)\n",
    "        data.extend(thisdata['data'])\n",
    "    except:\n",
    "        print(\"reacts didn't work at all!\")\n",
    "        #just return an empty dict here\n",
    "        return {}\n",
    "\n",
    "    if 'paging' in thisdata.keys():\n",
    "        while thisdata['paging'].get('next', False):\n",
    "            time.sleep(1)\n",
    "            batchnum += 1\n",
    "            print(\"reacts batch: \", batchnum)\n",
    "            try:\n",
    "                web_response = urllib2.urlopen(thisdata['paging']['next'])\n",
    "                thisdata = json.loads(web_response.read())\n",
    "                if thisdata['data']:\n",
    "                    data.extend(thisdata['data'])\n",
    "            except:\n",
    "                print(\"problem getting extra batches\")\n",
    "    reacts = {'data' : data}\n",
    "    return reacts\n",
    "    \"\"\"\n",
    "    post_url = new_createPostReactionsUrl(USER_ID, POST_ID, APP_ID, APP_SECRET)\n",
    "    opener = urllib2.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    urllib2.install_opener(opener)\n",
    "#     print(\"post_url\", post_url)\n",
    "    try:\n",
    "        web_response = urllib2.urlopen(post_url)\n",
    "        readable_page = web_response.read()\n",
    "        thisdata = json.loads(readable_page)\n",
    "    except:\n",
    "        print(\"reacts didn't work at all!\")\n",
    "        return None\n",
    "\n",
    "    return thisdata['summary']['total_count']\n",
    "\n",
    "def getPostComments(USER_ID, POST_ID, n = 1):\n",
    "    \"\"\"returns a JSON of a post's comments\"\"\"\n",
    "    data = []\n",
    "    post_url = createPostCommentsUrl(USER_ID, POST_ID, APP_ID, APP_SECRET)\n",
    "    opener = urllib2.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    urllib2.install_opener(opener)\n",
    "    batchnum = 1\n",
    "    time.sleep(n)\n",
    "\n",
    "    try:\n",
    "        web_response = urllib2.urlopen(post_url)\n",
    "        readable_page = web_response.read()\n",
    "        thisdata = json.loads(readable_page)\n",
    "        data.extend(thisdata['data'])\n",
    "    except:\n",
    "        print(\"comments didn't work at all!\")\n",
    "        #in this case, just return an empty dict\n",
    "        return {}\n",
    "\n",
    "    if 'paging' in thisdata.keys():\n",
    "        while thisdata['paging'].get('next', False):\n",
    "            time.sleep(n)\n",
    "            batchnum += 1\n",
    "            print(\"comments batch: \", batchnum)\n",
    "            try:\n",
    "                web_response = urllib2.urlopen(thisdata['paging']['next'])\n",
    "                thisdata = json.loads(web_response.read())\n",
    "                if thisdata['data']:\n",
    "                    data.extend(thisdata['data'])\n",
    "            except:\n",
    "                print(\"problem getting extra batches\")\n",
    "    comments = {\"data\" : data}\n",
    "    return comments\n",
    "\n",
    "def getPostAttachments(USER_ID, POST_ID, n = 1):\n",
    "    \"\"\"returns a JSON of a post's attachments\"\"\"\n",
    "    post_url = createPostAttachmentsUrl(USER_ID, POST_ID, APP_ID, APP_SECRET)\n",
    "    time.sleep(n)\n",
    "    web_response = urllib2.urlopen(post_url)\n",
    "    readable_page = web_response.read()\n",
    "    return json.loads(readable_page)\n",
    "\n",
    "\n",
    "\n",
    "APP_ID = \"495004908666220\"\n",
    "APP_SECRET = \"72b126cb77822118f4fd073882c191ac\"\n",
    "\n",
    "buzzfeed = []\n",
    "# If you like, you can change how long the program waits after\n",
    "# each API call here by setting n = (number of seconds). To be\n",
    "# safe and follow Facebook's conditions, we set n = 1 by default.\n",
    "# Change at your own risk.\n",
    "#\n",
    "# NOTE: This has been changed from 1 to 20 due to ANOTHER change in\n",
    "# the Facebook Graph API.\n",
    "n = 20\n",
    "\n",
    "#make a dict to relate outlet names to their url repn's\n",
    "outlet_urls = {\"ABC_News_Politics\" : 'abc',\n",
    "           \"Addicting_Info\" : 'addictinginfo',\n",
    "           \"CNN_Politics\" : \"cnn\",\n",
    "           \"Eagle_Rising\" : \"eaglerising\",\n",
    "           \"Freedom_Daily\" : \"freedomdaily\",\n",
    "           \"Occupy_Democrats\" : \"occupydemocrats\",\n",
    "           \"Politico\" : \"politi.co\",\n",
    "           \"Right_Wing_News\" : \"rightwingnews\",\n",
    "           \"The_Other_98%\" : \"TheOther98\" }\n",
    "\n",
    "#first load the buzzfeed data\n",
    "buzzfeed_file = 'facebook-fact-check.csv'\n",
    "with open(buzzfeed_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        buzzfeed.append(row)\n",
    "\n",
    "del buzzfeed[0]\n",
    "#now go thru each article\n",
    "for article in buzzfeed:\n",
    "    if article:\n",
    "        print(\"Article: \", article)\n",
    "        USER_ID = article[0]\n",
    "        POST_ID = article[1]\n",
    "        url = article[4]\n",
    "        outlet = article[3].replace(\" \", \"_\")\n",
    "        article_type = article[6]\n",
    "\n",
    "        print(\"working on \" + POST_ID)\n",
    "        #get post\n",
    "#         time.sleep(n)\n",
    "#         postJson = getPost(USER_ID, POST_ID)\n",
    "#         print(\"postJson: \", postJson)\n",
    "        try:\n",
    "            time.sleep(n)\n",
    "            postJson = getPost(USER_ID, POST_ID)\n",
    "            print(\"postJson: \", postJson)\n",
    "            postJson['type'] = article_type\n",
    "            first_party = True if outlet_urls[outlet] in postJson.get('link') else False\n",
    "            postJson['first_party'] = first_party\n",
    "        except:\n",
    "            print(\"post error\")\n",
    "            postJson = {}\n",
    "        #get number of reactions\n",
    "#         postJson['reacts'] = getPostReactions(USER_ID, POST_ID)\n",
    "        #get comments and reacts\n",
    "#         time.sleep(n)\n",
    "#         commentJson = getPostComments(USER_ID, POST_ID, n)\n",
    "\n",
    "#         #get attachments\n",
    "#         try:\n",
    "#             attachJson = getPostAttachments(USER_ID, POST_ID, n)\n",
    "#         except:\n",
    "#             print(\"attach\")\n",
    "#             attachJson = {}\n",
    "\n",
    "        wd = 'dataset/'\n",
    "        if not os.path.isdir('./dataset/'):\n",
    "            os.mkdir('dataset')\n",
    "        postfile = wd + outlet + '/' + POST_ID + '/posts.json'\n",
    "        commentfile = wd + outlet + '/' + POST_ID + '/comments.json'\n",
    "        attachfile = wd + outlet + '/' + POST_ID + '/attach.json'\n",
    "#             print(wd + outlet + '/' + POST_ID + '/posts.json')\n",
    "        os.makedirs(wd + outlet + '/' + POST_ID)\n",
    "#             os.makedirs(commentfile)\n",
    "#             os.makedirs(attachfile)\n",
    "#             os.system('touch ' + postfile)\n",
    "#             os.system('touch ' + commentfile)\n",
    "#             os.system('touch ' + attachfile)\n",
    "\n",
    "        with open(postfile, 'w') as f:\n",
    "            json.dump(postJson, f, indent = 4, sort_keys = True)\n",
    "\n",
    "#         with open(commentfile, 'w') as f2:\n",
    "#             json.dump(commentJson, f2, indent = 4, sort_keys = True)\n",
    "\n",
    "#         with open(attachfile, 'w') as f4:\n",
    "#             json.dump(attachJson, f4, indent = 4, sort_keys = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "As it turns out, dealing with the facebook API has advanced within those 4 years. Additionally, many of the posts which are being scraped, have been removed already, which seems to affect a larger part of the already underrepresented posts of the \"mostly fake\" type.\n",
    "\n",
    "Conclusion: The \"BuzzFeed-Webis Fake News Corpus 2016\" should be prefered over the BuzzFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
