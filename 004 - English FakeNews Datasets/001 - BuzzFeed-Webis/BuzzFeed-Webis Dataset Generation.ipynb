{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a copypaste from the Datasets README:\n",
    "\n",
    "BuzzFeed-Webis Fake News Corpus 2016\n",
    "====================================\n",
    "The corpus comprises the output of 9 publishers in a week close to the US elections. Among the selected publishers are 6 prolific hyperpartisan ones\n",
    "(three left-wing and three right-wing), and three mainstream publishers (see Table 1). All publishers earned Facebookâ€™s blue checkmark, indicating authenticity and an elevated status within the network. For seven weekdays (September 19 to 23 and September 26 and 27), every post and linked news article of the 9 publishers was fact-checked by professional journalists at BuzzFeed. In total, 1,627 articles were checked, 826 mainstream, 256 left-wing and 545 right-wing. The imbalance between categories results from differing publication frequencies.\n",
    "\n",
    "\n",
    "The corpus comes with the following files:\n",
    "\n",
    "##### README.txt\n",
    "\n",
    "This file.\n",
    "\n",
    "##### web-archives/*.warc\n",
    "\n",
    "The web archive files that contain the HTTP messages that where sent and received during the crawl\n",
    "\n",
    "##### articles/*.xml \n",
    "\n",
    "The articles extracted from the web archive files in XML format with annotations.\n",
    "\n",
    "##### schema.xsd\n",
    "\n",
    "Schema of the article files with explanations of the used XML tags. Can be used with object binding libraries (like JAXB) to parse the XML.\n",
    "\n",
    "##### overview.csv\n",
    "\n",
    "Giving the portal, orientation, veracity, and URL for each article. The same data is also contained in the XML files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will only use the articles xml-files as they contain mainText, title as well as veracity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets have a quick look at the basic functionality of ElementTree in conjunction with one of the XML files\n",
    "\n",
    "analogue to: https://towardsdatascience.com/extracting-information-from-xml-files-into-a-pandas-dataframe-11f32883ce45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "path = 'articles/'\n",
    "files = os.listdir(path)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_file1 = os.path.join(path, files[0])\n",
    "tree = ET.parse(file_path_file1)\n",
    "root = tree.getroot()\n",
    "print(root.tag, root.attrib)\n",
    "for child in root:     \n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets generate a dataframe from the XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing elements: mainText/title/veracity 0 0 0\n",
      "dataframe shape:  (1627, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mainText</th>\n",
       "      <th>title</th>\n",
       "      <th>veracity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With the Hillary Clinton-Donald Trump debates ...</td>\n",
       "      <td>The Impact of Debates? It's Debatable</td>\n",
       "      <td>mostly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As police today captured the man wanted for qu...</td>\n",
       "      <td>Details Emerge About NYC Bomb Suspect Ahmad Kh...</td>\n",
       "      <td>mostly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day after explosive devices were discovere...</td>\n",
       "      <td>Donald Trump Repeats Calls for Police Profilin...</td>\n",
       "      <td>mostly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahmad Khan Rahami, earlier named a person of i...</td>\n",
       "      <td>NY, NJ Bombings Suspect Charged With Attempted...</td>\n",
       "      <td>mostly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump's surrogates and leading supporte...</td>\n",
       "      <td>Trump Surrogates Push Narrative That Clinton S...</td>\n",
       "      <td>mostly true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            mainText  \\\n",
       "0  With the Hillary Clinton-Donald Trump debates ...   \n",
       "1  As police today captured the man wanted for qu...   \n",
       "2  One day after explosive devices were discovere...   \n",
       "3  Ahmad Khan Rahami, earlier named a person of i...   \n",
       "4  Donald Trump's surrogates and leading supporte...   \n",
       "\n",
       "                                               title     veracity  \n",
       "0              The Impact of Debates? It's Debatable  mostly true  \n",
       "1  Details Emerge About NYC Bomb Suspect Ahmad Kh...  mostly true  \n",
       "2  Donald Trump Repeats Calls for Police Profilin...  mostly true  \n",
       "3  NY, NJ Bombings Suspect Charged With Attempted...  mostly true  \n",
       "4  Trump Surrogates Push Narrative That Clinton S...  mostly true  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_news = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for file in files:\n",
    "    file_path=path+file\n",
    "    #print('Processing....'+file_path)\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # keep track of missing elements in the xml tree\n",
    "    mainText_missing = 0\n",
    "    title_missing = 0\n",
    "    veracity_missing = 0\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    if root.find('mainText') != None:\n",
    "        data_dict['mainText'] = root.find('mainText').text\n",
    "    else:\n",
    "        data_dict['mainText'] = ''\n",
    "        mainText_missing += 1\n",
    "            \n",
    "        \n",
    "    if root.find('title') != None:\n",
    "        data_dict['title'] = root.find('title').text\n",
    "    else:\n",
    "        data_dict['title'] = ''\n",
    "        title_missing += 1\n",
    "        \n",
    "    if root.find('veracity') != None:\n",
    "        data_dict['veracity'] = root.find('veracity').text\n",
    "    else:\n",
    "        data_dict['veracity'] = ''\n",
    "        veracity_missing += 1\n",
    "   \n",
    "    \n",
    "    df_news = pd.concat([df_news, pd.DataFrame(data_dict,index=[i])])\n",
    "    i=i+1\n",
    "        \n",
    "print(\"missing elements: mainText/title/veracity\", mainText_missing, title_missing, veracity_missing)\n",
    "\n",
    "# peek at the head of the dataframe and its shape\n",
    "print(\"dataframe shape: \", df_news.shape)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets clean up the dataset\n",
    "\n",
    "By having a quick look at the produced csv with e.g. \"CSViewer\" we can quickly tell the following:\n",
    "\n",
    "- Some entries do not have a text and/or title. We want to drop those that do not have a text.\n",
    "- Titles are very short in comparison to the texts, and both will be concatinated at a later point, so a missing title can be overlooked, while a missing text cannot.\n",
    "- Very few texts are actually very short, but still longer than a title.\n",
    "- Some entries have \"The document has moved here.\" as text and \"Moved Permanently\" as title, with a random veracity assigned. We want to drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shape before cleaning:  (1627, 3)\n",
      "dataframe shape after removing no-text entries:  (1604, 3)\n",
      "dataframe shape after 'The document has moved here.' entries (1590, 3)\n",
      "dataframe shape after 'Moved Permanently' entries (1590, 3)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"dataframe shape before cleaning: \", df_news.shape)\n",
    "\n",
    "# remove entries with empty mainText or with  \"The document has moved here.\" as text or \"Moved Permanently\" as title\n",
    "df_news['mainText'].replace('', np.nan, inplace=True)\n",
    "df_news.dropna(subset=['mainText'], inplace=True)\n",
    "print(\"dataframe shape after removing no-text entries: \", df_news.shape)\n",
    "\n",
    "\n",
    "# remove entries with \"The document has moved here.\" as text\n",
    "df_news['mainText'].replace('The document has moved here.', np.nan, inplace=True)\n",
    "df_news.dropna(subset=['mainText'], inplace=True)\n",
    "print(\"dataframe shape after 'The document has moved here.' entries\", df_news.shape)\n",
    "\n",
    "\n",
    "# remove entries with \"Moved Permanently\" as title\n",
    "df_news['mainText'].replace('Moved Permanently', np.nan, inplace=True)\n",
    "df_news.dropna(subset=['mainText'], inplace=True)\n",
    "print(\"dataframe shape after 'Moved Permanently' entries\", df_news.shape)\n",
    "\n",
    "# Convert NaN titles to an empty string for later concatination of title and text\n",
    "# print(df_news['title'].isnull().values.any())\n",
    "df_news[['title']] = df_news[['title']].fillna('')\n",
    "# print(df_news['title'].isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, analogue to Reis et al.'s process in \"Supervised Learning for Fake News Detection\" do the following:\n",
    "\n",
    "Quote: \"we discarded stories labeled as 'non factual content' and merged those labeled as 'mostly false' and 'mixture of true and false' into a single class, henceforth refered as 'fake news'. The reamining stories correspond to the 'true' portion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values before ['mostly true' 'no factual content' 'mixture of true and false'\n",
      " 'mostly false']\n",
      "unique values after [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values before conversion\n",
    "uniques = df_news['veracity'].unique()\n",
    "print(\"unique values before\", uniques)\n",
    "\n",
    "\n",
    "# convert\n",
    "df_news['veracity'] = df_news['veracity'].map({'mixture of true and false': 1, 'mostly false': 1, 'mostly true': 0})\n",
    "\n",
    "# non factual content is now \"nan\", which can be used to discard these entries\n",
    "df_news.dropna(subset=['veracity'], inplace=True)\n",
    "\n",
    "# the veracity column is turned from float to int\n",
    "df_news['veracity'] = df_news['veracity'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Check the unique values after conversion\n",
    "uniques = df_news['veracity'].unique()\n",
    "print(\"unique values after\", uniques)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lastly, save the dataframe as .csv for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for future use\n",
    "df_news.to_csv('BuzzFeed-Webis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_news['title'].isnull().values.any())\n",
    "print(df_news['mainText'].isnull().values.any())\n",
    "print(df_news['veracity'].isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
