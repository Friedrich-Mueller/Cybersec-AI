{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As part of my contribution to the seminar, aside from my presentation covering the paper \"Supervised Learning for Fake News Detection\" by Reis et al., a written essay about the paper would have resulted in mostly restating the extensive procedures for the extraction of 141 textual features, 5 news source features and 21 environment features. Hence I decided to attempt to reproduce the work programmatically. \n",
    "\n",
    "The main focus of this work is on the extraction of the textual features. The reason for focusing only on the extraction of textual features that is that ultimately, the work can be used to classify german fake news, for which only two datasets currently exist, which do not provide the information required for extracting news source features or environment features.\n",
    "\n",
    "So the goal of this work is to see how far you can get, using only the textual features and to compare the results based on the two mention german datasets as well as with the initially used dataset used in the work of Reis et al. with their results from the paper, as well as with the performance of a somewhat conventional classifier, which uses simple count vectors in regards to the features extraction.\n",
    "\n",
    "As we will later see, the english data used by Reis et al. is not easily recovered. Hence the main focus of this notebook will be on german data, the extraction of features of german data and comparing simple count vector based classification with extensive feature extraction based classification, as well as comparing the results of the extensive feature extraction performed here with the results of Reis et al.\n",
    "Additionally we will see that there is a dataset thats similiar in nature but very small, the BuzzFeed-Webis dataset. In fact it is so small, that the simple feature extraction based classifiers fail miserably in identifying fake news better than 'guessing'.\n",
    "\n",
    "Since the extensive feature extraction took alot of work, it is only being done here for german data. Hence, as mentioned, the comparison of feature extraction methods will only focus on german data. (The classification attempts with the Buzzfeed-Webis dataset using simple feature extraction are still to be found in this notebook in nonetheless, as part of a reflection of efforts invested into this work)\n",
    "\n",
    "For the classification we will look at XGBoost and RF classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Step 0 - Imports\n",
    "\n",
    "### Step 1 - Gathering data\n",
    "\n",
    "#### Step 1.1 - German data\n",
    "\n",
    "As mentioned, there are only two german dataset. Those are \n",
    "\n",
    "- GermanFakeNC: German Fake News Corpus (https://zenodo.org/record/3375714#.Ya8IHtDMKUk)\n",
    "- Kaggle Starter: Fake News Dataset German 9cc110a2-9 (https://www.kaggle.com/kerneler/starter-fake-news-dataset-german-9cc110a2-9/data )\n",
    "\n",
    "#### Step 1.1.1 - Scraping of GermanFakeNC articles\n",
    "\n",
    "- Scraping and generation of the GermanFakeNC dataset\n",
    "\n",
    "#### Step 1.1.2 - Preparation and merge of datasets\n",
    "\n",
    "- preparing and aligning the two german datasets\n",
    "- merging of datasets\n",
    "\n",
    "#### Step 1.2 - English data\n",
    "\n",
    "Optimally, we want to use the original dataset which was used in the work of Reis et al., \"Supervised Learning for Fake News Detection\". That is:\n",
    "\n",
    "- BuzzFace: A News Veracity Dataset with Facebook User Commentary and Ego (https://metatext.io/datasets/buzzface)\n",
    "\n",
    "As we will see in the progress of this work, acquiring that dataset in the shape that it was originally used is not possible. Hence another dataset is being used which is, not only with respect to size, not as extensive, but very close to the BuzzFace dataset in nature:\n",
    "\n",
    "- BuzzFeed-Webis Fake News Corpus 16 (https://webis.de/data/buzzfeed-webis-fake-news-16.html)\n",
    "\n",
    "#### Step 1.2.1 - Attempts of generating the BuzzFace dataset\n",
    "\n",
    "- showcase of failed attempt\n",
    "\n",
    "#### Step 1.2.2 - Generating the BuzzFeed-Webis dataset\n",
    "\n",
    "- Construction of the BuzzFeed-Webis dataset from XML files\n",
    "\n",
    "### Step 2 - Performing the extensive (textual only) feature extration as proposed by Reis et al.\n",
    "\n",
    "- Step-by-Step reconstruction of textual features\n",
    "\n",
    "### Step 3 - Running the data (german only) on the two best classifiers (XGBoost and RFs) used in Reis et al.'s work\n",
    "\n",
    "- Using XGBoost and RFs to compare the performance of the merged german dataset to the english (BuzzFeed-Webis) dataset\n",
    "\n",
    "#### Step 3.1 - Using stop words, stemming and  count vectorization\n",
    "\n",
    "#### Step 3.1.1 - XGBoost\n",
    "\n",
    "#### Step 3.1.1.1 - german dataset\n",
    "\n",
    "#### Step 3.1.1.2 - english dataset\n",
    "\n",
    "\n",
    "#### Step 3.1.2 - Random Forest\n",
    "\n",
    "#### Step 3.1.2.1 - german dataset\n",
    "\n",
    "#### Step 3.1.2.2 - english dataset\n",
    "\n",
    "\n",
    "\n",
    "#### Step 3.2 - Using the extensive feature extraction as proposed by Reis et al.\n",
    "\n",
    "#### Step 3.2.1 - XGBoost\n",
    "\n",
    "#### Step 3.2.1.1 - german dataset\n",
    "\n",
    "\n",
    "\n",
    "#### Step 3.2.2 - Random Forest\n",
    "\n",
    "#### Step 3.2.2.1 - german dataset\n",
    "\n",
    "\n",
    "\n",
    "### Step 4 - Overview of the results\n",
    "\n",
    "### Step 5 - Conclusion\n",
    "\n",
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import py7zr\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, recall_score, precision_score\n",
    "import nltk\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "import dagshub\n",
    "import copy\n",
    "from joblib import dump\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus.reader.tagged as tagged\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from ClassifierBasedGermanTagger.ClassifierBasedGermanTagger import ClassifierBasedGermanTagger\n",
    "import random\n",
    "import textstat\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import string \n",
    "import numpy as np\n",
    "\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "\n",
    "from googleapiclient import discovery\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Gathering data\n",
    "\n",
    "### Step 1.1 - German data\n",
    "\n",
    "### Step 1.1.1 - Scraping of GermanFakeNC articles\n",
    "\n",
    "This uses the GermanFakeNC and crawls through its entries to receive title and body from each samples URL\n",
    "\n",
    "Credit goes to https://dagshub.com/leudom/german-fake-news-classifier/src/master/src/data/scrape_news.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract title\n",
    "def extract_title(url):\n",
    "    article = Article(url)\n",
    "    try:\n",
    "        article.download()\n",
    "        logger.info('Article title downloaded from %s' % url)\n",
    "        article.parse()\n",
    "    except:\n",
    "        article.title = 'No title'\n",
    "\n",
    "    return article.title\n",
    "\n",
    "# Function to extract text\n",
    "def extract_text(url):\n",
    "    article = Article(url)\n",
    "    try:\n",
    "        article.download()\n",
    "        logger.info('Article text downloaded from %s' % url)\n",
    "        article.parse()\n",
    "    except:\n",
    "        article.text = 'No text'\n",
    "\n",
    "    return article.text\n",
    "\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "with py7zr.SevenZipFile('german_datasets.7z', 'r') as z:\n",
    "    z.extractall()\n",
    "\n",
    "df = pd.read_json(\"GermanFakeNC.json\")\n",
    "\n",
    "logger.info('Head of dataframe: \\n%s' % df.head())\n",
    "\n",
    "# Get title and text to each samples url\n",
    "df['titel'] = df['URL'].apply(extract_title)\n",
    "df['text'] = df['URL'].apply(extract_text)\n",
    "\n",
    "logger.info('Head of dataframe after parsing: \\n%s' % df.head())\n",
    "\n",
    "# Filter rows with no information (titel or text)\n",
    "no_info_mask = (df['titel'] != 'No title') & (df['text'] != 'No text')\n",
    "df_final = df[no_info_mask]\n",
    "\n",
    "logger.info('Shape of final dataframe: %s' % str(df_final.shape))\n",
    "logger.info('dtypes: \\n%s' % str(df_final.dtypes))\n",
    "logger.info('Rows with null values: \\n%s' % df_final.isnull().sum())\n",
    "\n",
    "# Save as csv\n",
    "try:\n",
    "    df_final.to_csv('df_GermanFakeNC.csv', index=False)\n",
    "    logger.info(\"CSV was saved to disk\")\n",
    "except Exception:\n",
    "    logger.exception(\"Couldn't save CSV to disc \\n\", exc_info=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1.2 - Preparation and merge of datasets\n",
    "\n",
    "Credit goes to https://dagshub.com/leudom/german-fake-news-classifier/src/master/src/data/make_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_kegglenews_csv(filepath):\n",
    "    \"\"\" \n",
    "    1.) Drop columns -> Kategorie, Quelle, Art\n",
    "    2.) Check on duplicate Titel and Body and drop the first entry of duplicates\n",
    "    3.) Rename Columns in order to match it with the other dataset (GermanFakeNC)\n",
    "    4.) Add column source_name with news_csv to identifiy the source of a row after merging\n",
    "    \"\"\"\n",
    "\n",
    "    # Read news.csv from disk\n",
    "    _df = pd.read_csv(filepath)\n",
    "    logger.debug(_df.info())\n",
    "    # Drop cols\n",
    "    logger.info('Null values in news.csv: \\n%s' % _df.isnull().sum())\n",
    "    cols_to_drop = ['Kategorie', 'Quelle', 'Art']\n",
    "    _df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    logger.info('Cols %s dropped' % cols_to_drop)\n",
    "\n",
    "\n",
    "    # Drop duplicates\n",
    "    logger.info('Percent duplicated Titel and Body: \\n%s' % str(\n",
    "        _df.duplicated(subset=['Titel', 'Body']).value_counts(normalize=True)))\n",
    "    _df.drop_duplicates(subset=['Titel', 'Body'], inplace=True)\n",
    "    logger.info('Duplicates in Titel and Body dropped')\n",
    "\n",
    "    # Rename Cols\n",
    "    new_cols = {'id': 'src_id',\n",
    "                'Titel': 'title',\n",
    "                'Body': 'text',\n",
    "                'Datum': 'date',\n",
    "                'Fake': 'fake'}\n",
    "    _df.rename(columns=new_cols, inplace=True)\n",
    "    logger.info('Cols renamed')\n",
    "\n",
    "    # Add col source_name\n",
    "    _df['src_name'] = 'news_csv'\n",
    "\n",
    "    return _df\n",
    "\n",
    "\n",
    "def prepare_germanfakenc(filepath):\n",
    "    \"\"\" \n",
    "    1.) Drop columns -> [False_Statement_1_Location,\n",
    "                         False_Statement_1_Index,\n",
    "                         False_Statement_2_Location,\n",
    "                         False_Statement_2_Index,\n",
    "                         False_Statement_3_Location,\n",
    "                         False_Statement_3_Index,\n",
    "                         Ratio_of_Fake_Statements,\n",
    "                         Overall_Rating]\n",
    "        We treat all entries as fakenews, eventhough there are some instances\n",
    "        that have a very low fake overall ratings!!\n",
    "    2.) Make index source_id\n",
    "    3.) Check on duplicate titel and text and drop the first entry of duplicates\n",
    "    4.) Drop rows where titel or text is null \n",
    "    5.) Fill Dates for missing values -> From the URL we can see that the Date could\n",
    "        be 2017/12 \n",
    "    6.) Rename Columns in order to match it with the other dataset (news.csv)\n",
    "    7.) Add label col 'fake' = 1 -> all 1; col 'src_name' = 'GermanFakeNC'\n",
    "    \"\"\"\n",
    "\n",
    "    # Read news.csv from disk\n",
    "    _df = pd.read_csv(filepath)\n",
    "    \n",
    "    logger.debug(_df.info())\n",
    "    # Drop cols\n",
    "    logger.info('Null values in GermanFakeNC_interim.csv: \\n%s' % _df.isnull().sum())\n",
    "    cols_to_drop = ['False_Statement_1_Location',\n",
    "                    'False_Statement_1_Index',\n",
    "                    'False_Statement_2_Location',\n",
    "                    'False_Statement_2_Index',\n",
    "                    'False_Statement_3_Location',\n",
    "                    'False_Statement_3_Index',\n",
    "                    'Ratio_of_Fake_Statements',\n",
    "                    'Overall_Rating']\n",
    "    _df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    logger.info('Cols %s dropped' % cols_to_drop)\n",
    "\n",
    "    # Set source_id\n",
    "    _df.reset_index(inplace=True)\n",
    "    logger.info('Index reset')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    logger.info('Percent duplicated titel and text: \\n%s' % str(\n",
    "        _df.duplicated(subset=['titel', 'text']).value_counts(normalize=True)))\n",
    "    _df.drop_duplicates(subset=['titel', 'text'], inplace=True)\n",
    "    logger.info('Duplicates in titel and text dropped')\n",
    "\n",
    "    # Drop rows where titel or text is null\n",
    "    _df.dropna(subset=['titel', 'text'], inplace=True)\n",
    "    logger.info('Null rows for titel and text dropped')\n",
    "\n",
    "    # Fill the missing dates\n",
    "    _df['Date'].fillna(pd.to_datetime('01/12/2017'), inplace=True)\n",
    "\n",
    "    # Rename Cols\n",
    "    new_cols = {'index': 'src_id',\n",
    "                'titel': 'title',\n",
    "                'Date': 'date',\n",
    "                'URL': 'url'}\n",
    "    _df.rename(columns=new_cols, inplace=True)\n",
    "    logger.info('Cols renamed')\n",
    "\n",
    "    # Add col source_name\n",
    "    _df['fake'] = 1\n",
    "    _df['src_name'] = 'GermanFakeNC'\n",
    "\n",
    "    return _df\n",
    "\n",
    "def merge_datasets(df_1, df_2):\n",
    "    logger.info('Shape: %s\\n Columns: %s' % (df_1.shape, df_1.columns))\n",
    "    logger.info('Shape: %s\\n Columns: %s' % (df_2.shape, df_2.columns))\n",
    "    # Check col names\n",
    "    sym_diff = set(df_1).symmetric_difference(set(df_2))\n",
    "    assert len(sym_diff) == 0 , 'Differences in colnames of the two datasets'\n",
    "    return pd.concat([df_1, df_2], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s : %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "KEGGLENEWS_CSV = 'KeggleNews.csv'\n",
    "GERMAN_FAKE_NC = 'df_GermanFakeNC.csv'\n",
    "OUTPUT = 'german_datasets_merged.csv'\n",
    "\n",
    "df_news = prepare_kegglenews_csv(KEGGLENEWS_CSV)\n",
    "df_gfn = prepare_germanfakenc(GERMAN_FAKE_NC)\n",
    "df_merged = merge_datasets(df_news, df_gfn)\n",
    "\n",
    "\n",
    "# For this task here, we want to concat title and text into one and will save it in the text column\n",
    "\n",
    "# first load the pos tagger\n",
    "f = open('german_pos_classifier.pickle', 'rb')\n",
    "pos_clf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def title_to_sentence(title, clf):\n",
    "    tokens = clf.tag(nltk.word_tokenize(title))\n",
    "    if tokens[-1][1] != '$.':\n",
    "        return title+'.'\n",
    "    else:\n",
    "        return title\n",
    "df_merged['text'] = df_merged['title'].apply(title_to_sentence, clf=pos_clf)+' '+df_merged['text']\n",
    "\n",
    "\n",
    "try:\n",
    "    df_merged.to_csv(OUTPUT, sep=',', index=False)\n",
    "    logger.info('Final dataset prepared and saved to %s' % OUTPUT)\n",
    "except Exception:\n",
    "    logger.exception('File could not be daved to disk\\n', exc_info=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2 - English data\n",
    "\n",
    "Optimally, the same dataset as used in the original paper should be used, such that the only relevant metric for the performance is reduced to the implementation of the feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2.1 - Attempts of generating the BuzzFace dataset\n",
    "\n",
    "Unfortunately, the only way the BuzzFace dataset is available, is in the shape of a scraping framework. This means that if you attempted to scape the same news posts today, the dataset would turn out different, as alot of posts have vanished from the internet since 2016. Some manual lookup of links to posts provided in the framework verify that circumstance.\n",
    "\n",
    "Apart of that, the framework was written in python2 and includes linux bash calls. Those two circumstances can be worked around. On top of that, the framework utilizes facebooks graph API and a Disqus API. The former requires App ID and Secret ID keys, which, among other things, require a verified facebook account, for which peronal identification passes have to be handed in, which can be a rather big issue, depending on one's personal view when it comes to handing out personal information to the internet, specifically social media platforms like facebook.\n",
    "\n",
    "But as mentioned, even if all that is circumvented, the issue of scraping the web 6 years later still consists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2.2 - Generating the BuzzFeed-Webis dataset\n",
    "\n",
    "There happens to be a very similiar dataset, called the BuzzFeed-Webis dataset. It is very much alike as the BuzzFace dataset, except that it is alot smaller, but also fortunately comes as a consistent download and does not suffer from the requirement of scraping the data first.\n",
    "\n",
    "Here is a copypaste from the datasets README:\n",
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "### BuzzFeed-Webis Fake News Corpus 2016\n",
    "\n",
    "\n",
    "The corpus comprises the output of 9 publishers in a week close to the US elections. Among the selected publishers are 6 prolific hyperpartisan ones\n",
    "(three left-wing and three right-wing), and three mainstream publishers (see Table 1). All publishers earned Facebook’s blue checkmark, indicating authenticity and an elevated status within the network. For seven weekdays (September 19 to 23 and September 26 and 27), every post and linked news article of the 9 publishers was fact-checked by professional journalists at BuzzFeed. In total, 1,627 articles were checked, 826 mainstream, 256 left-wing and 545 right-wing. The imbalance between categories results from differing publication frequencies.\n",
    "\n",
    "\n",
    "The corpus comes with the following files:\n",
    "\n",
    "##### README.txt\n",
    "\n",
    "This file.\n",
    "\n",
    "##### web-archives/*.warc\n",
    "\n",
    "The web archive files that contain the HTTP messages that where sent and received during the crawl\n",
    "\n",
    "##### articles/*.xml \n",
    "\n",
    "The articles extracted from the web archive files in XML format with annotations.\n",
    "\n",
    "##### schema.xsd\n",
    "\n",
    "Schema of the article files with explanations of the used XML tags. Can be used with object binding libraries (like JAXB) to parse the XML.\n",
    "\n",
    "##### overview.csv\n",
    "\n",
    "Giving the portal, orientation, veracity, and URL for each article. The same data is also contained in the XML files.\n",
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "So, in short, the only file we need from this dataset is the articles folder, which consists one file for each sample in XML format. Here we will only use the articles xml-files as they contain mainText, title as well as veracity.\n",
    "\n",
    "Lets have a quick look at the basic functionality of ElementTree in conjunction with one of the XML files\n",
    "\n",
    "analogue to: https://towardsdatascience.com/extracting-information-from-xml-files-into-a-pandas-dataframe-11f32883ce45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'articles/'\n",
    "files = os.listdir(path)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_file1 = os.path.join(path, files[0])\n",
    "tree = ET.parse(file_path_file1)\n",
    "root = tree.getroot()\n",
    "print(root.tag, root.attrib)\n",
    "for child in root:     \n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets generate a dataframe from the XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for file in files:\n",
    "    file_path=path+file\n",
    "    #print('Processing....'+file_path)\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # keep track of missing elements in the xml tree\n",
    "    mainText_missing = 0\n",
    "    title_missing = 0\n",
    "    veracity_missing = 0\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    if root.find('mainText') != None:\n",
    "        data_dict['mainText'] = root.find('mainText').text\n",
    "    else:\n",
    "        data_dict['mainText'] = ''\n",
    "        mainText_missing += 1\n",
    "            \n",
    "        \n",
    "    if root.find('title') != None:\n",
    "        data_dict['title'] = root.find('title').text\n",
    "    else:\n",
    "        data_dict['title'] = ''\n",
    "        title_missing += 1\n",
    "        \n",
    "    if root.find('veracity') != None:\n",
    "        data_dict['veracity'] = root.find('veracity').text\n",
    "    else:\n",
    "        data_dict['veracity'] = ''\n",
    "        veracity_missing += 1\n",
    "   \n",
    "    \n",
    "    df_news = pd.concat([df_news, pd.DataFrame(data_dict,index=[i])])\n",
    "    i=i+1\n",
    "        \n",
    "print(\"missing elements: mainText/title/veracity\", mainText_missing, title_missing, veracity_missing)\n",
    "\n",
    "# peek at the head of the dataframe and its shape\n",
    "print(\"dataframe shape: \", df_news.shape)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets clean up the dataset\n",
    "\n",
    "By having a quick look at the produced csv with e.g. \"CSViewer\" we can quickly tell the following:\n",
    "\n",
    "- Some entries do not have a text and/or title. We want to drop those that do not have a text.\n",
    "- Titles are very short in comparison to the texts, and both will be concatinated at a later point, so a missing title can be overlooked, while a missing text cannot.\n",
    "- Very few texts are actually very short, but still longer than a title.\n",
    "- Some entries have \"The document has moved here.\" as text and \"Moved Permanently\" as title, with a random veracity assigned. We want to drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"dataframe shape before cleaning: \", df_news.shape)\n",
    "\n",
    "# remove entries with empty mainText or with  \"The document has moved here.\" as text or \"Moved Permanently\" as title\n",
    "df_news['mainText'].replace('', np.nan, inplace=True)\n",
    "df_news.dropna(subset=['mainText'], inplace=True)\n",
    "print(\"dataframe shape after removing no-text entries: \", df_news.shape)\n",
    "\n",
    "\n",
    "# remove entries with \"The document has moved here.\" as text\n",
    "df_news['mainText'].replace('The document has moved here.', np.nan, inplace=True)\n",
    "df_news.dropna(subset=['mainText'], inplace=True)\n",
    "print(\"dataframe shape after 'The document has moved here.' entries\", df_news.shape)\n",
    "\n",
    "\n",
    "# remove entries with \"Moved Permanently\" as title\n",
    "df_news['mainText'].replace('Moved Permanently', np.nan, inplace=True)\n",
    "df_news.dropna(subset=['mainText'], inplace=True)\n",
    "print(\"dataframe shape after 'Moved Permanently' entries\", df_news.shape)\n",
    "\n",
    "# Convert NaN titles to an empty string for later concatination of title and text\n",
    "# print(df_news['title'].isnull().values.any())\n",
    "df_news[['title']] = df_news[['title']].fillna('')\n",
    "# print(df_news['title'].isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, analogue to Reis et al.'s process in \"Supervised Learning for Fake News Detection\" do the following:\n",
    "\n",
    "Quote: \"we discarded stories labeled as 'non factual content' and merged those labeled as 'mostly false' and 'mixture of true and false' into a single class, henceforth refered as 'fake news'. The reamining stories correspond to the 'true' portion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values before conversion\n",
    "uniques = df_news['veracity'].unique()\n",
    "print(\"unique values before\", uniques)\n",
    "\n",
    "\n",
    "# convert\n",
    "df_news['veracity'] = df_news['veracity'].map({'mixture of true and false': 1, 'mostly false': 1, 'mostly true': 0})\n",
    "\n",
    "# non factual content is now \"nan\", which can be used to discard these entries\n",
    "df_news.dropna(subset=['veracity'], inplace=True)\n",
    "\n",
    "# the veracity column is turned from float to int\n",
    "df_news['veracity'] = df_news['veracity'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Check the unique values after conversion\n",
    "uniques = df_news['veracity'].unique()\n",
    "print(\"unique values after\", uniques)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lastly, save the dataframe as .csv for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for future use\n",
    "df_news.to_csv('BuzzFeed-Webis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to keep in mind:\n",
    "\n",
    "We now have a total of two dataset - one german and one english. Those being:\n",
    "\n",
    "- german_datasets_merged.csv\n",
    "- BuzzFeed-Webis.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Performing the extensive (textual only) feature extration of Reis et al.\n",
    "\n",
    "Reis et al. evaluated a total of 141 textual features. Although not described very much in detail which those are, a guidance is given, such that with some research and expertise, those can be somewhat accurately reconstructed.\n",
    "\n",
    "The 5 main categories of features extracted are the following:\n",
    "\n",
    "1. Language Features (Syntax)\n",
    "2. Lexical Features\n",
    "3. Psycholinguistic Features\n",
    "4. Semantic Features\n",
    "5. Subjectivity Features\n",
    "\n",
    "First, we implement all the nessecary functions for the individual feature type extractions. Following will be a mail function called \"feature_extraction()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 -  Language Features (Syntax)\n",
    "\n",
    "(We first look at an overview of extracted features and then implement them combined)\n",
    "\n",
    "The work of Reis et al. suggests a total of 31  sentence level features and an addition of some amount of text readability features. It is not very specified which those are. But we will take the information provided and try to accumulate a total of 31+X amount of features. These include:\n",
    "\n",
    "Sentence level features:\n",
    "1. mean words per sentence\n",
    "2. mean syllables per sentence\n",
    "3. part-of-speech tag - 29 features\n",
    "4. Function words per sentence (here: all tags that were not collected in POS-tagging)\n",
    "        \n",
    "Text readability features:\n",
    "\n",
    "5. Flesch reading ease: 206.835 - 1.015(total_words/total_sentences) - 84.6(total_syllables/total_words)\n",
    "6. Flesch-Kincaid grade level: 0.39(total_words/total_sentences) + 11.8(total_syllables/total_words) - 15.59\n",
    "\n",
    "Additionally, bag of word approaches and n-grams are mentioned for this part. These appear to be more fitting for the semantic features, such that we cover those in the later part.\n",
    "\n",
    "\n",
    "(It is important to note, that these features have to be normalized by the amount of sentences.)\n",
    "\n",
    "#### 2.1.1 - words per sentence\n",
    "\n",
    "This is a simple words per sentence count.\n",
    "\n",
    "#### 2.1.2 - syllables per sentence\n",
    "\n",
    "This is a simple syllables per sentence count using https://pypi.org/project/textstat/.\n",
    "\n",
    "#### 2.1.3 - POS-Tags\n",
    "\n",
    "analogue to: https://datascience.blog.wzb.eu/2016/07/13/accurate-part-of-speech-tagging-of-german-texts-with-nltk/\n",
    "\n",
    "Using ClassifierBasedGermanTagger from https://github.com/ptnplanet/NLTK-Contributions for up to 96% tagging accuracy\n",
    "\n",
    "Using the german TIGER Corpus - Tagset  can be found here https://www.ims.uni-stuttgart.de/documents/ressourcen/korpora/tiger-corpus/annotation/tiger_introduction.pdf\n",
    "\n",
    "Relevant tags mentioned in the paper from Reis et al. are \"nouns, verbs, adjectives, etc\". So for now we will try to stick those/such major categories:\n",
    "\n",
    "(Please refer to the above tiger_introduction.pdf for insight into the abbreviations)\n",
    "\n",
    "For adjectives:\n",
    "- ADJA (adjective attributive)\n",
    "- ADJD (adjective, adverbial or predicative)\n",
    "\n",
    "For nouns:\n",
    "- NN (common noun)\n",
    "- NE (proper noun)\n",
    "\n",
    "For verbs:\n",
    "- VVFIN (finite verb, full)\n",
    "- VVIMP (imperative, full)\n",
    "- VVINF (infinitive, full)\n",
    "- VVIZU (infinitive with 'zu', full)\n",
    "- VVPP (perfect particle, full)\n",
    "- VAFIN (finite verb, auxiliary)\n",
    "- VAIMP (imperative, auxiliary)\n",
    "- VAINF (infinitive, auxiliary)\n",
    "- VAPP (perfect particle, auxiliary)\n",
    "- VMFIN (finite verb, modal)\n",
    "- VMINF (infinite, modal)\n",
    "\n",
    "In regards to \"etc\", the following categories might make sense to include as well:\n",
    "\n",
    "Adverbs:\n",
    "- ADV (adverb)\n",
    "\n",
    "Anglicisms:\n",
    "- FM (foreign language material)\n",
    "\n",
    "Posessive pronouns:\n",
    "- PPOSAT (attributive possessive pronoun)\n",
    "\n",
    "Interrogative pronouns:\n",
    "- PWS (substituting interrogative pronoun)\n",
    "- PWAV (adverbial interrogative or relative pronoun)\n",
    "\n",
    "#### 2.1.4 - Function words per sentence\n",
    "\n",
    "These are all words per sentence, which are not tagged in 2.1.3\n",
    "\n",
    "#### 2.1.5 - Text readability features\n",
    "\n",
    "Flesch reading ease and Flesch-Kincaid grade using https://pypi.org/project/textstat/.\n",
    "\n",
    "\n",
    "Textstat offers a multitude of further readability metrics which might be worth looking into but might come with difficulties to implement, such that we leave it at these two metrics and a total of 25 lexical features\n",
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "Now lets get to the implementation of syntax based features (2.1)\n",
    "\n",
    "We begin by training a POS tagger. This part will ultimately be commented out, and the tagger will just be saved/loaded after a first execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparation for POS-Tagging\n",
    "\n",
    "# Set up the corpus - We use the german TIGER Corpus\n",
    "nltk.download('punkt')\n",
    "corp = nltk.corpus.ConllCorpusReader('.', 'german_tiger_train.conll',\n",
    "                                     ['ignore', 'words', 'ignore', 'ignore', 'pos'],\n",
    "                                     encoding='utf-8')\n",
    "\n",
    "# prepare the training of a Tagger\n",
    "tagged_sents = list(corp.tagged_sents())\n",
    "random.shuffle(tagged_sents)\n",
    "\n",
    "# set a split size: use 90% for training, 10% for testing\n",
    "split_perc = 0.1\n",
    "split_size = int(len(tagged_sents) * split_perc)\n",
    "train_sents, test_sents = tagged_sents[split_size:], tagged_sents[:split_size]\n",
    "\n",
    "# train the classifier\n",
    "tagger = ClassifierBasedGermanTagger(train=train_sents)\n",
    "\n",
    "accuracy = tagger.accuracy(test_sents)\n",
    "print(\"POS-Tagger trained accuracy\", accuracy)\n",
    "\n",
    "f = open('german_pos_classifier.pickle', 'wb')\n",
    "pickle.dump(tagger, f)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This here is just a dummy for testing, but parts of it will be recycled for the title to sentence transformation\n",
    "\n",
    "INPUTFILE = 'german_datasets_merged.csv'\n",
    "test_df = pd.read_csv(INPUTFILE, sep=',')\n",
    "\n",
    "test_df = test_df[:6]\n",
    "\n",
    "# print(test_df)\n",
    "\n",
    "\n",
    "# first load the pos tagger\n",
    "f = open('german_pos_classifier.pickle', 'rb')\n",
    "pos_clf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "def title_to_sentence(title, clf):\n",
    "    tokens = clf.tag(nltk.word_tokenize(title))\n",
    "    if tokens[-1][1] != '$.':\n",
    "        return title+'.'\n",
    "    else:\n",
    "        return title\n",
    "    \n",
    "    \n",
    "# test_df['title'] = test_df['title'].apply(title_to_sentence, clf=pos_clf)\n",
    "\n",
    "test_df['text'] = test_df['title'].apply(title_to_sentence, clf=pos_clf)+' '+test_df['text']\n",
    "\n",
    "# print(test_df['title'])\n",
    "test_df.to_csv('test_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def words_per_sentence(text):\n",
    "    words = len(text.split())\n",
    "    sentences = len(nltk.sent_tokenize(text))\n",
    "    return words/sentences\n",
    "\n",
    "def syllables_per_sentece(text):\n",
    "    syllables = textstat.syllable_count(text)\n",
    "    sentences = len(nltk.sent_tokenize(text))\n",
    "    return syllables/sentences\n",
    "\n",
    "def pos_tag_per_sentence(text, tags, clf):\n",
    "    sentences = len(nltk.sent_tokenize(text))\n",
    "    tagged_text = clf.tag(nltk.word_tokenize(text))\n",
    "    pos_counts = Counter(pos for _, pos in tagged_text)\n",
    "    counts = []\n",
    "    for tag in tags:\n",
    "        counts.append(pos_counts[tag])\n",
    "    # function words\n",
    "    func_words = len(tagged_text) - sum(counts)\n",
    "    counts.append(func_words)\n",
    "    counts = [x/sentences for x in counts]\n",
    "    # function words\n",
    "    return counts\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    words = len(text.split())\n",
    "    sentences = len(nltk.sent_tokenize(text))\n",
    "    syllables = textstat.syllable_count(text)\n",
    "    return 206.835-1.015*(words/sentences)-84.6*(syllables/words)\n",
    "\n",
    "def flesch_kincaid_grade(text):\n",
    "    words = len(text.split())\n",
    "    sentences = len(nltk.sent_tokenize(text))\n",
    "    syllables = textstat.syllable_count(text)\n",
    "    return 0.39*(words/sentences)+11.8*(syllables/words)-15.59\n",
    "\n",
    "def language_features(_df):\n",
    "  \n",
    "    vector = []\n",
    "    \n",
    "    ### 2.1.1 - words per sentence\n",
    "    print(\"words per sentence\")\n",
    "    series = _df['text'].apply(words_per_sentence)\n",
    "    series.name = 'wps'\n",
    "    vector.append(series)\n",
    "    \n",
    "    ### 2.1.2 - syllables per sentence\n",
    "    print(\"syllables per sentence\")\n",
    "    textstat.set_lang('de')\n",
    "    series = _df['text'].apply(syllables_per_sentece)\n",
    "    series.name = 'sps'\n",
    "    vector.append(series)\n",
    "    \n",
    "    ### 2.1.3 - POS-Tags per text\n",
    "    print(\"POS-tags\")\n",
    "    # load the pos tagger for POS-Tagging\n",
    "    f = open('german_pos_classifier.pickle', 'rb')\n",
    "    pos_clf = pickle.load(f)\n",
    "    f.close()\n",
    "    # relevant tags\n",
    "    tags = [\"ADJA\", \"ADJD\", \"NN\", \"NE\", \"VVFIN\", \"VVIMP\", \"VVINF\",\n",
    "            \"VVIZU\", \"VVPP\", \"VAFIN\", \"VAIMP\", \"VAINF\", \"VAPP\",\n",
    "            \"VMFIN\", \"VMINF\", \"ADV\", \"FM\", \"PPOSAT\", \"PWS\", \"PWAV\"]\n",
    "    series = _df['text'].apply(pos_tag_per_sentence, tags=tags, clf=pos_clf)\n",
    "    series_list = series.to_list()\n",
    "    series_numpy = np.array([np.array(x) for x in series_list]).T\n",
    "    for i in range(len(series_numpy)-1):\n",
    "        series = pd.Series(series_numpy[i])\n",
    "        series.name = tags[i]\n",
    "        vector.append(series)\n",
    "        \n",
    "    ### 2.1.4 - Function words per sentence\n",
    "    print(\"function words\")\n",
    "    series = pd.Series(series_numpy[-1])\n",
    "    series.name = 'fncwords'\n",
    "    vector.append(series)\n",
    "    \n",
    "    #### 2.1.5 - Text readability features\n",
    "    print(\"text readability\")\n",
    "    # Flesch reading ease\n",
    "    series = _df['text'].apply(flesch_reading_ease)\n",
    "    series.name = 'fre'\n",
    "    vector.append(series)\n",
    "    \n",
    "    # Flesch-Kincaid \n",
    "    series = _df['text'].apply(flesch_kincaid_grade)\n",
    "    series.name = 'fcg'\n",
    "    vector.append(series)\n",
    "\n",
    "    \n",
    "    print(\"Amount of language (syntax) features: \", len(vector))\n",
    "    return vector\n",
    "\n",
    "\n",
    "german_datasets_merged = pd.read_csv('german_datasets_merged.csv', sep=',')\n",
    "results = language_features(german_datasets_merged)\n",
    "# print(type(results))\n",
    "# for i in range(len(results)):\n",
    "#     print(results[i])\n",
    "\n",
    "# we dump the results for later use\n",
    "with open(\"language_features\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)\n",
    "    print(\"writing to file done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Lexical Features\n",
    "\n",
    "(We first look at an overview of extracted features and then implement them combined)\n",
    "\n",
    "The work of Reis et al. suggests an undisclosed amount of character and word-level features and again only mentions a few specifically, while refering two other works that refer other works etc. As this work's efforts during the extraction of language features have shown, it can require a very deep dive into the specifics. So from this point on, we will try to keep it as shallow as possible and as deep and reasonabe to stay within the extends of this contribution to the seminar.\n",
    "\n",
    "Reis et al. mentions amount of unique words and their frequency in the text, as well as features including number of words, first person pronouns, demonstrative pronouns, verbs, hashtags, punctuation counts, \"etc\". So we will try to focus on those who dont seem to overlap with the previous part and try to pick up as many as possible that surround those.\n",
    "\n",
    "#### 2.2.1 - Per word features\n",
    "\n",
    "These are simple characters, syllables, hashtags and punctuation per word counts. For some reason specifically hashtags were mentioned by Reis et al. Punctuation here includes: . ? ! , : ; ' \" - ( ) [ ] / \\ < >\n",
    "\n",
    "#### 2.2.2 - Frequency of large words\n",
    "\n",
    "According to Marian et al. in \"CLEARPOND: Cross-Linguistic Easy-Access Resource for Phonological and Orthographic Neighborhood Densities\" the average word length for german is around 8.25. So here we count all words longer than 12 characters (including a single character punctuation suffix in some rare cases) and divide by the total amount of words. The actual length to choose here is prone to change and the sloppyness with the occasional single character punctuation suffix might actually be neglectable but can also be fixed.\n",
    "\n",
    "#### 2.2.3 - Frequency of unique words\n",
    "\n",
    "Amount of words that occur exactly once in a text, divided by total amount of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characters_per_word(text):\n",
    "    characters = len(text)\n",
    "    words = len(text.split())\n",
    "    return words/characters\n",
    "\n",
    "def syllables_per_word(text):\n",
    "    syllables = textstat.syllable_count(text)\n",
    "    words = len(text.split())\n",
    "    return words/syllables\n",
    "\n",
    "def punctuation_per_word(text):\n",
    "    words = len(text.split())\n",
    "    punctuation = ['.', '?', '!', ',', ':', ';', '\\'', '\"', '-', '(', ')', '[', ']', '/', '\\\\', '<', '>' ]\n",
    "    count = 0\n",
    "    for c in punctuation:\n",
    "        count += text.count(c)\n",
    "    return count/words\n",
    "\n",
    "def hashtags_per_word(text):\n",
    "    words = len(text.split())\n",
    "    return text.count(\"#\")/words\n",
    "\n",
    "def large_word_freq(text):\n",
    "    words = len(text.split())\n",
    "    words_list = text.split()\n",
    "    count = 0\n",
    "    for w in words_list:\n",
    "        if len(w)>12:\n",
    "            count+=1\n",
    "    return count/words\n",
    "\n",
    "def unique_word_freq(text):\n",
    "    words = len(nltk.tokenize.word_tokenize(text))\n",
    "    words_list = nltk.tokenize.word_tokenize(text)\n",
    "    frequencies = nltk.FreqDist(words_list)\n",
    "    return len(frequencies.hapaxes())/words\n",
    "\n",
    "def lexical_features(_df):\n",
    "  \n",
    "    vector = []\n",
    "    \n",
    "    ### 2.2.1 - per word features\n",
    "    # characters per word\n",
    "    series = _df['text'].apply(characters_per_word)\n",
    "    series.name = 'cpw'\n",
    "    vector.append(series)\n",
    "    \n",
    "    # syllables per word\n",
    "    series = _df['text'].apply(syllables_per_word)\n",
    "    series.name = 'spw'\n",
    "    vector.append(series)   \n",
    "    \n",
    "    # punctuation per word\n",
    "    series = _df['text'].apply(punctuation_per_word)\n",
    "    series.name = 'ppw'\n",
    "    vector.append(series)    \n",
    "    \n",
    "    # hashtags per word\n",
    "    series = _df['text'].apply(hashtags_per_word)\n",
    "    series.name = 'hpw'\n",
    "    vector.append(series)    \n",
    "    \n",
    "    ### 2.2.2 - Frequency of large words\n",
    "    # characters per word\n",
    "    series = _df['text'].apply(large_word_freq)\n",
    "    series.name = 'lwf'\n",
    "    vector.append(series)\n",
    "    \n",
    "    ### 2.2.3 - Frequency of unique words\n",
    "    series = _df['text'].apply(unique_word_freq)\n",
    "    series.name = 'uwf'\n",
    "    vector.append(series)\n",
    " \n",
    "    print(\"Amount of lexical features: \", len(vector))\n",
    "    return vector\n",
    "\n",
    "german_datasets_merged = pd.read_csv('german_datasets_merged.csv', sep=',')\n",
    "results = lexical_features(german_datasets_merged)\n",
    "# print(type(results))\n",
    "# for i in range(len(results)):\n",
    "#     print(results[i])\n",
    "    \n",
    "    \n",
    "# we dump the results for later use\n",
    "with open(\"lexical_features\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)\n",
    "    print(\"writing to file done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 -  Psycholinguistic Features\n",
    "\n",
    "So, apparently the extraction of psycholinguistic features with the use of a library or tool named Linguistic Inquiry\n",
    "and Word Count (LIWC), requires the purches of said tool.\n",
    "\n",
    "The good news is, that there is actually a version specifically for german. So potentially, the feature extraction based on german fake news as suggested by Reis et al. is potentially not coming short in regards to psycholinguistic features (at least as you purchase the tool). There is a free/demo version available, although that is only accessable via browser and limited to a single text up to 1000 words at once. See https://fortext.net/tools/tools/liwc\n",
    "\n",
    "This, unfortunately, means that at this point these features have to be left out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for liwc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Semantic Features\n",
    "\n",
    "(We first look at an overview of extracted features and then implement them combined)\n",
    "\n",
    "Semantic features attempt to capture semantic meaning that a text can inherit. Reis et al. merely refered to three other works, and additionally mentioned the incorporation of a toxicity score as computed via an API of Google (https://www.perspectiveapi.com/#/).\n",
    "\n",
    "The referenced works suggest methods which in Reis et al.'s work has suggested to be part of the language features (Syntax) extraction, e.g. POS-related information retrieval. At the same time BoW- and n-gram-approach has been mentioned as part of the language features (syntax) extraction, while the two referenced works for the semantic features make it seem, as if that approach was aiming at extracting semantic meaning. Hence, the n-gram feature extraction will be done in this part here.\n",
    "\n",
    "\n",
    "#### 2.4.1 - N-gram features\n",
    "\n",
    "N-grams can be used as features for comment or review tagging. A dataset of (previously stemmed and stop-word-cleaned) comments or reviews can be disected into 2-grams and/or 3-grams, and the occurences of each gram is being counted. A list of the most commonly occuring grams is then being used to categorize (unseen) comments/reviews.\n",
    "\n",
    "So what we will do here is basically the same. We collect the top 10, 20 or 30 most common 2- and 3-grams over the whole dataset and use these similiar to a bag of words approach. Their counts within a text makes up the features. There might be inclination to collect these most common n-grams from both, the fake and the true news seperately, but here we do it for both combined. Yes, this means for an imbalanced dataset consisting mostly of not-fakenews that texts who do not include (many of) such n-grams, that they are likely fake. This has to be explored a bit deeper than just picking the solution we use here for now. \n",
    "\n",
    "#### 2.4.2 - Google API toxicity score\n",
    "\n",
    "This part required requesting a google API key. See https://developers.perspectiveapi.com/s/docs-enable-the-api.\n",
    "\n",
    "Additionally access to the API has to be requested. See https://support.perspectiveapi.com/s/docs-get-started\n",
    "\n",
    "Given the key and permission to access the api, we are able to query the API. See https://developers.perspectiveapi.com/s/docs-sample-requests\n",
    "\n",
    "Unfortunately, not enough API calls can be done (within a certain time only maybe) to justify using the toxicity score here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-gram preperation\n",
    "To perform the n-gram based feature extraction, we are first going to extract the 40 most common 2-grams and 3-grams from our german dataset.\n",
    "This has/had to be executed just once. The files are in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text, n, stop_words, snowball):\n",
    "    words_list = nltk.tokenize.word_tokenize(text)\n",
    "    punctuation = string.punctuation+\"``\"+\"''\"+'”'+'--'+'“'\n",
    "    filtered_sentence = [snowball.stem(w) for w in words_list if (not w.lower() in stop_words) and (not w in punctuation)]\n",
    "\n",
    "    n_grams = []\n",
    "    for i in range(len(filtered_sentence)-n+1):\n",
    "        n_gram = \"\"\n",
    "        for j in range(n):\n",
    "            if j>0:\n",
    "                n_gram += ' '+ filtered_sentence[i+j]\n",
    "            else:\n",
    "                n_gram += filtered_sentence[i]\n",
    "        n_grams.append(n_gram)\n",
    "    return n_grams\n",
    "\n",
    "# load the dataset\n",
    "ngram_df = pd.read_csv('german_datasets_merged.csv', sep=',')\n",
    "\n",
    "# load stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORD_LIST = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "# get the grams from the dataset \n",
    "result_2_grams = ngram_df['text'].apply(n_grams, n=2, stop_words=STOPWORD_LIST, snowball=SnowballStemmer(\"german\"))\n",
    "result_2_grams = [item for sublist in result_2_grams for item in sublist] \n",
    "most_common_2_grams = [i[0] for i in Counter(result_2_grams).most_common(40)]\n",
    "# saving for use\n",
    "with open(\"most_common_2_grams\", \"wb\") as fp:\n",
    "    pickle.dump(most_common_2_grams, fp)\n",
    "\n",
    "    \n",
    "result_3_grams = ngram_df['text'].apply(n_grams, n=3, stop_words=STOPWORD_LIST, snowball=SnowballStemmer(\"german\"))\n",
    "result_3_grams = [item for sublist in result_3_grams for item in sublist] \n",
    "most_common_3_grams = [i[0] for i in Counter(result_3_grams).most_common(40)]\n",
    "# saving for use\n",
    "with open(\"most_common_3_grams\", \"wb\") as fp:\n",
    "    pickle.dump(most_common_3_grams, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)\n",
    "\n",
    "def n_grams_occurences(text, n, stop_words, snowball):\n",
    "    words_list = nltk.tokenize.word_tokenize(text)\n",
    "    punctuation = string.punctuation+\"``\"+\"''\"+'”'+'--'+'“'\n",
    "    filtered_sentence = [snowball.stem(w) for w in words_list if (not w.lower() in stop_words) and (not w in punctuation)]\n",
    "\n",
    "    n_grams = []\n",
    "    for i in range(len(filtered_sentence)-n+1):\n",
    "        n_gram = \"\"\n",
    "        for j in range(n):\n",
    "            if j>0:\n",
    "                n_gram += ' '+ filtered_sentence[i+j]\n",
    "            else:\n",
    "                n_gram += filtered_sentence[i]\n",
    "        n_grams.append(n_gram)\n",
    "        \n",
    "    if n == 2:\n",
    "        with open(\"most_common_2_grams\", \"rb\") as fp:   # Unpickling\n",
    "            most_common_n_grams = pickle.load(fp)\n",
    "    else:\n",
    "        with open(\"most_common_3_grams\", \"rb\") as fp:   # Unpickling\n",
    "            most_common_n_grams = pickle.load(fp)  \n",
    "\n",
    "\n",
    "    occurences = 0\n",
    "    for n_g in n_grams:\n",
    "        if n_g in most_common_n_grams:\n",
    "            occurences += 1\n",
    "    return occurences/len(text.split())\n",
    "   \n",
    "\n",
    "\n",
    "def toxicity_score(text):\n",
    "    API_KEY = 'AIzaSyDFGkrmIjTxNzpTCmhxuIDSIxTvBqfXx6o'\n",
    "\n",
    "    client = discovery.build(\n",
    "      \"commentanalyzer\",\n",
    "      \"v1alpha1\",\n",
    "      developerKey=API_KEY,\n",
    "      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "      static_discovery=False,\n",
    "    )\n",
    "\n",
    "    analyze_request = {\n",
    "      'comment': { 'text': text },\n",
    "      'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    return response['attributeScores']['TOXICITY']['spanScores'][0]['score']['value']\n",
    "\n",
    "\n",
    "def semantic_features(_df):\n",
    "  \n",
    "    vector = []\n",
    "    \n",
    "    \n",
    "    ### 2.4.1 - N-gram features\n",
    "    # load stopwords\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORD_LIST = nltk.corpus.stopwords.words('german')\n",
    "    \n",
    "    # 2-grams feature\n",
    "    series = _df['text'].apply(n_grams_occurences, n=2, stop_words=STOPWORD_LIST, snowball=SnowballStemmer(\"german\"))\n",
    "    series.name = '2grm'\n",
    "    vector.append(series)\n",
    "    \n",
    "    # 3-grams feature\n",
    "    series = _df['text'].apply(n_grams_occurences, n=3, stop_words=STOPWORD_LIST, snowball=SnowballStemmer(\"german\"))\n",
    "    series.name = '3grm'\n",
    "    vector.append(series)\n",
    "    \n",
    "    ### 2.4.2 - Google API toxicity score\n",
    "    # characters per word\n",
    "#     series = _df['text'].apply(toxicity_score)\n",
    "#     series.name = 'tox'\n",
    "#     vector.append(series)\n",
    "\n",
    "    print(\"Amount of semantic features: \", len(vector))\n",
    "    return vector\n",
    "\n",
    "german_datasets_merged = pd.read_csv('german_datasets_merged.csv', sep=',')\n",
    "results = semantic_features(german_datasets_merged)\n",
    "# print(type(results))\n",
    "# for i in range(len(results)):\n",
    "#     print(results[i])\n",
    "     \n",
    "# we dump the results for later use\n",
    "with open(\"semantic_features\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)\n",
    "    print(\"writing to file done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Subjectivity Features\n",
    "\n",
    "(We first look at an overview of extracted features and then implement them combined)\n",
    "\n",
    "In regards to the final category of features, Reis et al. state the usage of the TextBlob API for the extraction of subjectivity and sentiment scores of a text. The german extension of said API does not support sentiment assessment and subjectivity scores yet, unfortunately. See https://langui.ch/nlp/python/textblob-de-dev/#fn1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Running the data on the two best classifiers (XGBoost and RFs) used in Reis et al.'s work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 - Using stop words, stemming and  count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overide CountVectorizer to integrate stemming\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def __init__(self, stemmer, **kwargs):\n",
    "        super(StemmedCountVectorizer, self).__init__(**kwargs)\n",
    "        self.stemmer = stemmer\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (self.stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        params = super().get_params(deep=deep)\n",
    "        cp = copy.copy(self)\n",
    "        cp.__class__ = CountVectorizer\n",
    "        params.update(CountVectorizer.get_params(cp, deep))\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1.1 - XGBoost\n",
    "\n",
    "#### Step 3.1.1.1 - german dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"FakeNewsClassifier Trainer\n",
    "\n",
    "This script conduct the following steps (overview):\n",
    "\n",
    "1. Load the processes dataset as dataframe\n",
    "2. Concatenate title and text in one column\n",
    "3. Split the dataset into train and test\n",
    "\n",
    "4. Set up a sklearn pipeline with preprocessor and classifier for the combined column (title+text)\n",
    "   and perform hyperparam search \n",
    "5. Extract best score and best_params for and log it\n",
    "6. Predict on testset and log the score\n",
    "\n",
    "7. Pickle best pipe to disk (final model)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#%% Load and split dataset\n",
    "INPUTFILE = 'german_datasets_merged.csv'\n",
    "df = pd.read_csv(INPUTFILE, sep=',')\n",
    "logger.info('Distribution of fake news in entire dataset: \\n%s' % df.fake.value_counts(normalize=True))\n",
    "\n",
    "X = df['title']+' '+df['text']\n",
    "y = df['fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                    test_size=0.2,\n",
    "                    stratify=y,\n",
    "                    random_state=42)\n",
    "\n",
    "# Download and init stopwordlist\n",
    "nltk.download('stopwords')\n",
    "STOPWORD_LIST = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "#%% Define param grid for tuning\n",
    "param_grid = {'vectorizer__max_features':[500],\n",
    "        'clf__n_estimators': [400],\n",
    "        'clf__learning_rate': [0.9],\n",
    "        'clf__max_depth': [5]}\n",
    "\n",
    "# Constants for training\n",
    "MODEL_PATH = 'model.pkl'\n",
    "CV_SCORING = 'f1'\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# %%\n",
    "with dagshub.dagshub_logger() as dagslog:\n",
    "        # Pipeline\n",
    "        pipe = Pipeline(steps=[\n",
    "        ('vectorizer', StemmedCountVectorizer(stop_words=STOPWORD_LIST,\n",
    "                                   token_pattern=r'\\b[a-zA-Z]{2,}\\b',\n",
    "                                   stemmer=GermanStemmer(ignore_stopwords=True),\n",
    "                                   lowercase=True)),\n",
    "#         ('clf', CatBoostClassifier(allow_writing_files=False)),\n",
    "        ('clf', XGBClassifier())\n",
    "            \n",
    "        ])\n",
    "        \n",
    "        # pipe_title_tune.get_params().keys()\n",
    "        logger.info('Start training estimator pipeline')\n",
    "        cv_grid = GridSearchCV(pipe, param_grid, scoring=CV_SCORING, cv=CV_FOLDS, n_jobs=2)\n",
    "        cv_grid.fit(X_train, y_train)\n",
    "        #cv_grid.best_estimator_.named_steps['clf'].get_all_params()\n",
    "\n",
    "        # Log and assign best score and params to var\n",
    "        logger.info('Best params from GridSearchCV: %s' % str(cv_grid.best_params_))\n",
    "        dagslog.log_hyperparams({'model_path': MODEL_PATH})\n",
    "        dagslog.log_hyperparams({'cv_score': CV_SCORING})\n",
    "        dagslog.log_hyperparams({'best_cv_params': cv_grid.best_params_})\n",
    "\n",
    "        logger.info('Best %s from GridSearchCV: %s' % (CV_SCORING, str(cv_grid.best_score_)))\n",
    "        dagslog.log_metrics({'best_cv_score': cv_grid.best_score_})\n",
    "        \n",
    "        # Predict on testdata\n",
    "        y_pred = cv_grid.best_estimator_.predict(X_test)\n",
    "        logger.info('f1_score for testdata: %s' % str(f1_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'f1_score_on_testdata': f1_score(y_test, y_pred)})\n",
    "        logger.info('precision_score for testdata: %s' % str(precision_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'precision_score_on_testdata': precision_score(y_test, y_pred)})\n",
    "        logger.info('recall_score for testdata: %s' % str(recall_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'recall_score_on_testdata': recall_score(y_test, y_pred)})\n",
    "        logger.info('Classification report for testdata: \\n%s' % classification_report(y_test, y_pred))\n",
    "        logger.info('Confusion matrix for testdata: \\n%s' % confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # Dump model to disk\n",
    "#         dump(cv_grid.best_estimator_, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1.1.2 - english dataset\n",
    "\n",
    "This classification run can be ignored as described at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load and split dataset\n",
    "INPUTFILE = 'BuzzFeed-Webis.csv'\n",
    "df = pd.read_csv(INPUTFILE, sep=',', keep_default_na=False)\n",
    "logger.info('Distribution of fake news in entire dataset: \\n%s' % df.veracity.value_counts(normalize=True))\n",
    "\n",
    "X = df['title']+' '+df['mainText']\n",
    "y = df['veracity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                    test_size=0.2,\n",
    "                    stratify=y,\n",
    "                    random_state=42)\n",
    "\n",
    "# Download and init stopwordlist\n",
    "nltk.download('stopwords')\n",
    "STOPWORD_LIST = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#%% Define param grid for tuning\n",
    "param_grid = {'vectorizer__max_features':[500],\n",
    "        'clf__n_estimators': [400],\n",
    "        'clf__learning_rate': [0.9],\n",
    "        'clf__max_depth': [5]}\n",
    "\n",
    "# Constants for training\n",
    "MODEL_PATH = 'model.pkl'\n",
    "CV_SCORING = 'f1'\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# %%\n",
    "with dagshub.dagshub_logger() as dagslog:\n",
    "        # Pipeline\n",
    "        pipe = Pipeline(steps=[\n",
    "        ('vectorizer', StemmedCountVectorizer(stop_words=STOPWORD_LIST,\n",
    "                                   token_pattern=r'\\b[a-zA-Z]{2,}\\b',\n",
    "                                   stemmer=GermanStemmer(ignore_stopwords=True),\n",
    "                                   lowercase=True)),\n",
    "#         ('clf', CatBoostClassifier(allow_writing_files=False)),\n",
    "        ('clf', XGBClassifier())\n",
    "            \n",
    "        ])\n",
    "        \n",
    "        # pipe_title_tune.get_params().keys()\n",
    "        logger.info('Start training estimator pipeline')\n",
    "        cv_grid = GridSearchCV(pipe, param_grid, scoring=CV_SCORING, cv=CV_FOLDS, n_jobs=2)\n",
    "        cv_grid.fit(X_train, y_train)\n",
    "        #cv_grid.best_estimator_.named_steps['clf'].get_all_params()\n",
    "\n",
    "        # Log and assign best score and params to var\n",
    "        logger.info('Best params from GridSearchCV: %s' % str(cv_grid.best_params_))\n",
    "        dagslog.log_hyperparams({'model_path': MODEL_PATH})\n",
    "        dagslog.log_hyperparams({'cv_score': CV_SCORING})\n",
    "        dagslog.log_hyperparams({'best_cv_params': cv_grid.best_params_})\n",
    "\n",
    "        logger.info('Best %s from GridSearchCV: %s' % (CV_SCORING, str(cv_grid.best_score_)))\n",
    "        dagslog.log_metrics({'best_cv_score': cv_grid.best_score_})\n",
    "        \n",
    "        # Predict on testdata\n",
    "        y_pred = cv_grid.best_estimator_.predict(X_test)\n",
    "        logger.info('f1_score for testdata: %s' % str(f1_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'f1_score_on_testdata': f1_score(y_test, y_pred)})\n",
    "        logger.info('precision_score for testdata: %s' % str(precision_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'precision_score_on_testdata': precision_score(y_test, y_pred)})\n",
    "        logger.info('recall_score for testdata: %s' % str(recall_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'recall_score_on_testdata': recall_score(y_test, y_pred)})\n",
    "        logger.info('Classification report for testdata: \\n%s' % classification_report(y_test, y_pred))\n",
    "        logger.info('Confusion matrix for testdata: \\n%s' % confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # Dump model to disk\n",
    "#         dump(cv_grid.best_estimator_, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1.2 - Random Forest\n",
    "\n",
    "#### Step 3.1.2.1 - german dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load and split dataset\n",
    "INPUTFILE = 'german_datasets_merged.csv'\n",
    "df = pd.read_csv(INPUTFILE, sep=',')\n",
    "logger.info('Distribution of fake news in entire dataset: \\n%s' % df.fake.value_counts(normalize=True))\n",
    "\n",
    "X = df['title']+' '+df['text']\n",
    "y = df['fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                    test_size=0.2,\n",
    "                    stratify=y,\n",
    "                    random_state=42)\n",
    "\n",
    "# Download and init stopwordlist\n",
    "nltk.download('stopwords')\n",
    "STOPWORD_LIST = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "#%% Define param grid for tuning\n",
    "param_grid = {'vectorizer__max_features':[500],\n",
    "        'clf__n_estimators': [400],\n",
    "#         'clf__learning_rate': [0.9],\n",
    "        'clf__max_depth': [25]}\n",
    "\n",
    "# Constants for training\n",
    "# MODEL_PATH = os.getenv('MODEL_PATH')\n",
    "MODEL_PATH = 'model.pkl'\n",
    "CV_SCORING = 'f1'\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# %%\n",
    "with dagshub.dagshub_logger() as dagslog:\n",
    "        # Pipeline\n",
    "        pipe = Pipeline(steps=[\n",
    "        ('vectorizer', StemmedCountVectorizer(stop_words=STOPWORD_LIST,\n",
    "                                   token_pattern=r'\\b[a-zA-Z]{2,}\\b',\n",
    "                                   stemmer=GermanStemmer(ignore_stopwords=True),\n",
    "                                   lowercase=True)),\n",
    "        ('clf', RandomForestClassifier())\n",
    "        ])\n",
    "        \n",
    "        # pipe_title_tune.get_params().keys()\n",
    "        logger.info('Start training estimator pipeline')\n",
    "        cv_grid = GridSearchCV(pipe, param_grid, scoring=CV_SCORING, cv=CV_FOLDS, n_jobs=2)\n",
    "        cv_grid.fit(X_train, y_train)\n",
    "        #cv_grid.best_estimator_.named_steps['clf'].get_all_params()\n",
    "\n",
    "        # Log and assign best score and params to var\n",
    "        logger.info('Best params from GridSearchCV: %s' % str(cv_grid.best_params_))\n",
    "        dagslog.log_hyperparams({'model_path': MODEL_PATH})\n",
    "        dagslog.log_hyperparams({'cv_score': CV_SCORING})\n",
    "        dagslog.log_hyperparams({'best_cv_params': cv_grid.best_params_})\n",
    "\n",
    "        logger.info('Best %s from GridSearchCV: %s' % (CV_SCORING, str(cv_grid.best_score_)))\n",
    "        dagslog.log_metrics({'best_cv_score': cv_grid.best_score_})\n",
    "        \n",
    "        # Predict on testdata\n",
    "        y_pred = cv_grid.best_estimator_.predict(X_test)\n",
    "        logger.info('f1_score for testdata: %s' % str(f1_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'f1_score_on_testdata': f1_score(y_test, y_pred)})\n",
    "        logger.info('precision_score for testdata: %s' % str(precision_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'precision_score_on_testdata': precision_score(y_test, y_pred)})\n",
    "        logger.info('recall_score for testdata: %s' % str(recall_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'recall_score_on_testdata': recall_score(y_test, y_pred)})\n",
    "        logger.info('Classification report for testdata: \\n%s' % classification_report(y_test, y_pred))\n",
    "        logger.info('Confusion matrix for testdata: \\n%s' % confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # Dump model to disk\n",
    "#         dump(cv_grid.best_estimator_, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1.2.2 - english dataset\n",
    "\n",
    "This classification run can be ignored as described at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load and split dataset\n",
    "INPUTFILE = 'BuzzFeed-Webis.csv'\n",
    "df = pd.read_csv(INPUTFILE, sep=',', keep_default_na=False)\n",
    "logger.info('Distribution of fake news in entire dataset: \\n%s' % df.veracity.value_counts(normalize=True))\n",
    "\n",
    "X = df['title']+' '+df['mainText']\n",
    "y = df['veracity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                    test_size=0.2,\n",
    "                    stratify=y,\n",
    "                    random_state=42)\n",
    "\n",
    "# Download and init stopwordlist\n",
    "nltk.download('stopwords')\n",
    "STOPWORD_LIST = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#%% Define param grid for tuning\n",
    "param_grid = {'vectorizer__max_features':[200,500,700],\n",
    "        'clf__n_estimators': [300, 400, 500],\n",
    "#         'clf__learning_rate': [0.9],\n",
    "        'clf__max_depth': [5, 10, 15]}\n",
    "\n",
    "# Constants for training\n",
    "MODEL_PATH = 'model.pkl'\n",
    "CV_SCORING = 'f1'\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# %%\n",
    "with dagshub.dagshub_logger() as dagslog:\n",
    "        # Pipeline\n",
    "        pipe = Pipeline(steps=[\n",
    "        ('vectorizer', StemmedCountVectorizer(stop_words=STOPWORD_LIST,\n",
    "                                   token_pattern=r'\\b[a-zA-Z]{2,}\\b',\n",
    "                                   stemmer=GermanStemmer(ignore_stopwords=True),\n",
    "                                   lowercase=True)),\n",
    "        ('clf', RandomForestClassifier())\n",
    "        ])\n",
    "        \n",
    "        # pipe_title_tune.get_params().keys()\n",
    "        logger.info('Start training estimator pipeline')\n",
    "        cv_grid = GridSearchCV(pipe, param_grid, scoring=CV_SCORING, cv=CV_FOLDS, n_jobs=2)\n",
    "        cv_grid.fit(X_train, y_train)\n",
    "        #cv_grid.best_estimator_.named_steps['clf'].get_all_params()\n",
    "\n",
    "        # Log and assign best score and params to var\n",
    "        logger.info('Best params from GridSearchCV: %s' % str(cv_grid.best_params_))\n",
    "        dagslog.log_hyperparams({'model_path': MODEL_PATH})\n",
    "        dagslog.log_hyperparams({'cv_score': CV_SCORING})\n",
    "        dagslog.log_hyperparams({'best_cv_params': cv_grid.best_params_})\n",
    "\n",
    "        logger.info('Best %s from GridSearchCV: %s' % (CV_SCORING, str(cv_grid.best_score_)))\n",
    "        dagslog.log_metrics({'best_cv_score': cv_grid.best_score_})\n",
    "        \n",
    "        # Predict on testdata\n",
    "        y_pred = cv_grid.best_estimator_.predict(X_test)\n",
    "        logger.info('f1_score for testdata: %s' % str(f1_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'f1_score_on_testdata': f1_score(y_test, y_pred)})\n",
    "        logger.info('precision_score for testdata: %s' % str(precision_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'precision_score_on_testdata': precision_score(y_test, y_pred)})\n",
    "        logger.info('recall_score for testdata: %s' % str(recall_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'recall_score_on_testdata': recall_score(y_test, y_pred)})\n",
    "        logger.info('Classification report for testdata: \\n%s' % classification_report(y_test, y_pred))\n",
    "        logger.info('Confusion matrix for testdata: \\n%s' % confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # Dump model to disk\n",
    "#         dump(cv_grid.best_estimator_, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 - Using the extensive feature extraction as proposed by Reis et al.\n",
    "\n",
    "#### Extract the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_extensive():\n",
    "    \n",
    "    with open(\"language_features\", \"rb\") as fp:   # Unpickling\n",
    "        language_features = pickle.load(fp)\n",
    "        \n",
    "    with open(\"lexical_features\", \"rb\") as fp:   # Unpickling\n",
    "        lexical_features = pickle.load(fp)\n",
    "        \n",
    "    with open(\"semantic_features\", \"rb\") as fp:   # Unpickling\n",
    "        semantic_features = pickle.load(fp)\n",
    "\n",
    "    feats_all = language_features + lexical_features + semantic_features\n",
    "    \n",
    "\n",
    "    print(\"Amount of all features: \", len(feats_all))\n",
    "#     print(type(feats_all)\n",
    "#     print(feats_all)\n",
    "\n",
    "#     dataframe = pd.DataFrame.from_items([(s.name, s) for s in feats_all])\n",
    "    dataframe = pd.concat(feats_all, axis=1)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "results = feature_extraction_extensive()\n",
    "print(type(results))\n",
    "print(results)\n",
    "\n",
    "# Save as csv\n",
    "try:\n",
    "    results.to_csv('german_datasets_merged_feats_extracted.csv', index=False)\n",
    "    logger.info(\"CSV was saved to disk\")\n",
    "except Exception:\n",
    "    logger.exception(\"Couldn't save CSV to disc \\n\", exc_info=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2.1 - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2.1.1 - german dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extracted = pd.read_csv(\"german_datasets_merged_feats_extracted.csv\")\n",
    "data_with_labels = pd.read_csv(\"german_datasets_merged.csv\")\n",
    "\n",
    "X = data_extracted\n",
    "y = data_with_labels['fake']\n",
    "# print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                    test_size=0.2,\n",
    "                    stratify=y)\n",
    "\n",
    "\n",
    "#%% Define param grid for tuning\n",
    "param_grid = {\n",
    "        'clf__n_estimators': [900],\n",
    "        'clf__learning_rate': [0.5],\n",
    "        'clf__max_depth': [4]}\n",
    "\n",
    "# Constants for training\n",
    "# MODEL_PATH = os.getenv('MODEL_PATH')\n",
    "MODEL_PATH = 'model.pkl'\n",
    "CV_SCORING = 'f1'\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# %%\n",
    "with dagshub.dagshub_logger() as dagslog:\n",
    "        # Pipeline\n",
    "        pipe = Pipeline(steps=[\n",
    "        ('clf', XGBClassifier())\n",
    "        ])\n",
    "        \n",
    "        # pipe_title_tune.get_params().keys()\n",
    "        logger.info('Start training estimator pipeline')\n",
    "        cv_grid = GridSearchCV(pipe, param_grid, scoring=CV_SCORING, cv=CV_FOLDS, n_jobs=2)\n",
    "        cv_grid.fit(X_train, y_train)\n",
    "        #cv_grid.best_estimator_.named_steps['clf'].get_all_params()\n",
    "\n",
    "        # Log and assign best score and params to var\n",
    "        logger.info('Best params from GridSearchCV: %s' % str(cv_grid.best_params_))\n",
    "        dagslog.log_hyperparams({'model_path': MODEL_PATH})\n",
    "        dagslog.log_hyperparams({'cv_score': CV_SCORING})\n",
    "        dagslog.log_hyperparams({'best_cv_params': cv_grid.best_params_})\n",
    "\n",
    "        logger.info('Best %s from GridSearchCV: %s' % (CV_SCORING, str(cv_grid.best_score_)))\n",
    "        dagslog.log_metrics({'best_cv_score': cv_grid.best_score_})\n",
    "        \n",
    "        # Predict on testdata\n",
    "#         y_pred = cv_grid.best_estimator_.predict(X_test)\n",
    "\n",
    "        # adept threshold\n",
    "        threshold = 0.001\n",
    "        y_pred = cv_grid.best_estimator_.predict_proba(X_test)\n",
    "        y_pred = (y_pred [:,1] >= threshold).astype('int')\n",
    "\n",
    "        logger.info('f1_score for testdata: %s' % str(f1_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'f1_score_on_testdata': f1_score(y_test, y_pred)})\n",
    "        logger.info('precision_score for testdata: %s' % str(precision_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'precision_score_on_testdata': precision_score(y_test, y_pred)})\n",
    "        logger.info('recall_score for testdata: %s' % str(recall_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'recall_score_on_testdata': recall_score(y_test, y_pred)})\n",
    "        logger.info('Classification report for testdata: \\n%s' % classification_report(y_test, y_pred))\n",
    "        logger.info('Confusion matrix for testdata: \\n%s' % confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2.2 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2.2.1 - german dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extracted = pd.read_csv(\"german_datasets_merged_feats_extracted.csv\")\n",
    "data_with_labels = pd.read_csv(\"german_datasets_merged.csv\")\n",
    "\n",
    "X = data_extracted\n",
    "y = data_with_labels['fake']\n",
    "# print(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                    test_size=0.2,\n",
    "                    stratify=y)\n",
    "\n",
    "\n",
    "#%% Define param grid for tuning\n",
    "param_grid = {\n",
    "        'clf__n_estimators': [1000],\n",
    "#         'clf__learning_rate': [0.8, 0.9, 1,],\n",
    "        'clf__max_depth': [30]}\n",
    "\n",
    "# Constants for training\n",
    "# MODEL_PATH = os.getenv('MODEL_PATH')\n",
    "MODEL_PATH = 'model.pkl'\n",
    "CV_SCORING = 'f1'\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# %%\n",
    "with dagshub.dagshub_logger() as dagslog:\n",
    "        # Pipeline\n",
    "        pipe = Pipeline(steps=[\n",
    "        ('clf', RandomForestClassifier())\n",
    "        ])\n",
    "        \n",
    "        # pipe_title_tune.get_params().keys()\n",
    "        logger.info('Start training estimator pipeline')\n",
    "        cv_grid = GridSearchCV(pipe, param_grid, scoring=CV_SCORING, cv=CV_FOLDS, n_jobs=2)\n",
    "        cv_grid.fit(X_train, y_train)\n",
    "        #cv_grid.best_estimator_.named_steps['clf'].get_all_params()\n",
    "\n",
    "        # Log and assign best score and params to var\n",
    "        logger.info('Best params from GridSearchCV: %s' % str(cv_grid.best_params_))\n",
    "        dagslog.log_hyperparams({'model_path': MODEL_PATH})\n",
    "        dagslog.log_hyperparams({'cv_score': CV_SCORING})\n",
    "        dagslog.log_hyperparams({'best_cv_params': cv_grid.best_params_})\n",
    "\n",
    "        logger.info('Best %s from GridSearchCV: %s' % (CV_SCORING, str(cv_grid.best_score_)))\n",
    "        dagslog.log_metrics({'best_cv_score': cv_grid.best_score_})\n",
    "        \n",
    "        # Predict on testdata\n",
    "#         y_pred = cv_grid.best_estimator_.predict(X_test)\n",
    "        \n",
    "        # adept threshold\n",
    "        threshold = 0.05\n",
    "        y_pred = cv_grid.best_estimator_.predict_proba(X_test)\n",
    "        y_pred = (y_pred [:,1] >= threshold).astype('int')\n",
    "        \n",
    "        logger.info('f1_score for testdata: %s' % str(f1_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'f1_score_on_testdata': f1_score(y_test, y_pred)})\n",
    "        logger.info('precision_score for testdata: %s' % str(precision_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'precision_score_on_testdata': precision_score(y_test, y_pred)})\n",
    "        logger.info('recall_score for testdata: %s' % str(recall_score(y_test, y_pred)))\n",
    "        dagslog.log_metrics({'recall_score_on_testdata': recall_score(y_test, y_pred)})\n",
    "        logger.info('Classification report for testdata: \\n%s' % classification_report(y_test, y_pred))\n",
    "        logger.info('Confusion matrix for testdata: \\n%s' % confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Overview of the results\n",
    "\n",
    "Here we will have a look at the performance of the two feature extraction methods on the german dataset(s). Additionally we will compare with the results of Reis et al. during their work.\n",
    "\n",
    "### Stop words, stemming and Count vectorizer:\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "    INFO:root:f1_score for testdata:        0.8695652173913043\n",
    "    INFO:root:precision_score for testdata: 0.8820403825717322\n",
    "    INFO:root:recall_score for testdata:    0.8574380165289256\n",
    "    INFO:root:Classification report for testdata: \n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99     11599\n",
    "           1       0.88      0.86      0.87       968\n",
    "\n",
    "    accuracy                           0.98     12567\n",
    "    macro avg      0.94      0.92      0.93     12567\n",
    "    weighted avg   0.98      0.98      0.98     12567\n",
    "\n",
    "    INFO:root:Confusion matrix for testdata: \n",
    "    [[11488   111]\n",
    "     [  138   830]]\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "    INFO:root:f1_score for testdata:        0.74\n",
    "    INFO:root:precision_score for testdata: 0.9367088607594937\n",
    "    INFO:root:recall_score for testdata:    0.6115702479338843\n",
    "    INFO:root:Classification report for testdata: \n",
    "    \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      1.00      0.98     11599\n",
    "           1       0.94      0.61      0.74       968\n",
    "\n",
    "    accuracy                           0.97     12567\n",
    "    macro avg      0.95      0.80      0.86     12567\n",
    "    weighted avg   0.97      0.97      0.96     12567\n",
    "\n",
    "    INFO:root:Confusion matrix for testdata: \n",
    "    [[11559    40]\n",
    "     [  376   592]]\n",
    "\n",
    "### Extensive feature extraction:\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "    INFO:root:f1_score for testdata: 0.7669515669515669\n",
    "    INFO:root:precision_score for testdata: 0.855146124523507\n",
    "    INFO:root:recall_score for testdata: 0.6952479338842975\n",
    "    INFO:root:Classification report for testdata: \n",
    "    \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.99      0.98     11599\n",
    "           1       0.86      0.70      0.77       968\n",
    "\n",
    "    accuracy                           0.97     12567\n",
    "    macro avg      0.92      0.84      0.87     12567\n",
    "    weighted avg   0.97      0.97      0.97     12567\n",
    "\n",
    "    INFO:root:Confusion matrix for testdata: \n",
    "    [[11485   114]\n",
    "     [  295   673]]\n",
    "\n",
    "#### Rand Forests\n",
    "\n",
    "    INFO:root:Best f1 from GridSearchCV: 0.6263095978894254\n",
    "    INFO:root:f1_score for testdata: 0.6285714285714286\n",
    "    INFO:root:precision_score for testdata: 0.8808193668528864\n",
    "    INFO:root:recall_score for testdata: 0.48863636363636365\n",
    "    INFO:root:Classification report for testdata: \n",
    "    \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.99      0.98     11599\n",
    "           1       0.88      0.49      0.63       968\n",
    "\n",
    "    accuracy                           0.96     12567\n",
    "    macro avg      0.92      0.74      0.80     12567\n",
    "    weighted avg   0.95      0.96      0.95     12567\n",
    "\n",
    "    INFO:root:Confusion matrix for testdata: \n",
    "    [[11535    64]\n",
    "     [  495   473]]\n",
    "\n",
    "\n",
    "### After adjusting the threshold\n",
    "\n",
    "Here we can see, that after adjusting the threshold, we can also get pretty close to 100% fake news detection, at the cost of an increasing amount of false positive detections.\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "    INFO:root:f1_score for testdata: 0.579064587973274\n",
    "    INFO:root:precision_score for testdata: 0.41839080459770117\n",
    "    INFO:root:recall_score for testdata: 0.9400826446280992\n",
    "    INFO:root:Classification report for testdata: \n",
    "    \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.89      0.94     11599\n",
    "           1       0.42      0.94      0.58       968\n",
    "\n",
    "    accuracy                           0.89     12567\n",
    "    macro avg      0.71      0.92      0.76     12567\n",
    "    weighted avg   0.95      0.89      0.91     12567\n",
    "\n",
    "    INFO:root:Confusion matrix for testdata: \n",
    "    [[10334  1265]\n",
    "     [   58   910]]\n",
    "\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "    INFO:root:Best f1 from GridSearchCV: 0.6269333490045463\n",
    "    INFO:root:f1_score for testdata: 0.3999138116785176\n",
    "    INFO:root:precision_score for testdata: 0.2526545058535257\n",
    "    INFO:root:recall_score for testdata: 0.9586776859504132\n",
    "    INFO:root:Classification report for testdata: \n",
    "    \n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.76      0.86     11599\n",
    "           1       0.25      0.96      0.40       968\n",
    "\n",
    "    accuracy                           0.78     12567\n",
    "    macro avg      0.62      0.86      0.63     12567\n",
    "    weighted avg   0.94      0.78      0.83     12567\n",
    "\n",
    "    INFO:root:Confusion matrix for testdata: \n",
    "    [[8854 2745]\n",
    "     [  40  928]]\n",
    "\n",
    "\n",
    "### Reis et al. results\n",
    "\n",
    "Reis et al. report macro F1 scores of ~0.81 for both RF as well as XGBoost. Furthermore, similiar to the results here, they adjusted the threshold to get near 100% detection of fake news at the cost of 40% false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Conclusion\n",
    "\n",
    "Unfortunately, a lot of potential of the extensive feature extraction was lost to the lack of detailed description of which features were used exactly how. On top of that, unfortunately the inavailability of LIWC and TextBlob for german resulted in having to dismiss 2 of the 5 feature categories suggested by Reis et al. One could make an effort and try to implement the ideas behind these tools manually, but attempting to do so would have blown up the extend of this seminar work.\n",
    "\n",
    "This left us with only 33 out of the suggested total of 141 features, showcasing how hard it is to actually perform the suggested extensive feature extraction as proposed.\n",
    "\n",
    "Although the low amount of 33 features out of 141 suggested ones might make it seem as if this attempt was lacklustered, but the intend of this seminar work was not to dive into every finely grained aspect of NLP of the last 30 years to archive a full and complete reconstruction of these 141 features, but instead, to dive into the difficulty and extend of said feature extraction. I hope this goal was reasonably archived.\n",
    "\n",
    "On a positive note, we can see that even with this downscaled version of Reis et al.'s feature extraction, and by adapting the treshold a detection of near 100% of fake news seems already possible, with only about 12,5% false positive for XGBoost and about 30% false positive with Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "For a list of references, please have a look inside the \"References\" folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "313px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
