{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As part of my contribution to the seminar, aside from my presentation covering the paper \"Supervised Learning for Fake News Detection\" by Reis et al., a written essay about the paper would have resulted in mostly restating the extensive procedures for the extraction of 141 textual features, 5 news source features and 21 environment features. Hence I decided to attempt to reproduce the work programmatically. \n",
    "\n",
    "The main focus of this work is on the extraction of the textual features. The reason for that is that ultimately, the work can be used to classify german fake news, for which only two datasets currently exist, which do not provide the data required for extracting news source features or environment features.\n",
    "\n",
    "So the goal of this work is to see how far you can get, using only the textual features and to compare the results based on the two mention german datasets as well as with the initially used dataset used in the work of Reis et al. with their results from the paper, as well as with the performance of a somewhat conventional classifier, which uses simple count vectors in regards to the features extraction.\n",
    "\n",
    "For the conventional classifier, a german fake news classifier from Dominik Leuziger is being used. (https://dagshub.com/leudom/german-fake-news-classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Gathering the data\n",
    "\n",
    "#### Step 1.1 - German data\n",
    "\n",
    "As mentioned, there are only two german dataset. Those are \n",
    "\n",
    "- Kaggle Starter: Fake News Dataset German 9cc110a2-9 (https://www.kaggle.com/kerneler/starter-fake-news-dataset-german-9cc110a2-9/data )\n",
    "- GermanFakeNC: German Fake News Corpus (https://zenodo.org/record/3375714#.Ya8IHtDMKUk)\n",
    "\n",
    "\n",
    "#### Step 1.2 - English data\n",
    "\n",
    "Optimally, we want to use the original dataset which was used in the work of Reis et al., \"Supervised Learning for Fake News Detection\". That is:\n",
    "\n",
    "- BuzzFace: A News Veracity Dataset with Facebook User Commentary and Ego (https://metatext.io/datasets/buzzface)\n",
    "\n",
    "As we will see in the progress of this work, acquiring that dataset in the shape that it was originally used is not possible. Hence another dataset is being used which is not as extensive, but very close to the BuzzFace dataset:\n",
    "\n",
    "- BuzzFeed-Webis Fake News Corpus 16 (https://webis.de/data/buzzfeed-webis-fake-news-16.html)\n",
    "\n",
    "#### Step 2 - Running the data (english and german) on a Catboost classifier with conventional feature extraction (count vectors)\n",
    "\n",
    "\n",
    "\n",
    "#### Step 3 - Performing the extensive (textual only) feature extration as proposed by Reis et al.\n",
    "\n",
    "\n",
    "\n",
    "#### Step 4 - Running the data (english and german) on the five classifiers used by Reis et al. with the extensive feature extraction on \n",
    "\n",
    "\n",
    "\n",
    "#### Step 5 - Comparison of the feature extration methods\n",
    "\n",
    "\n",
    "TODO: Run everything only on XGBoost --> replace Catboost with XGBoost, use only XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
