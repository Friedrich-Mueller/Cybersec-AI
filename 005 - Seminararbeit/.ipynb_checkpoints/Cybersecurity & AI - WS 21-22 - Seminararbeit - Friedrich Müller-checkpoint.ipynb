{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As part of my contribution to the seminar, aside from my presentation covering the paper \"Supervised Learning for Fake News Detection\" by Reis et al., a written essay about the paper would have resulted in mostly restating the extensive procedures for the extraction of 141 textual features, 5 news source features and 21 environment features. Hence I decided to attempt to reproduce the work programmatically. \n",
    "\n",
    "The main focus of this work is on the extraction of the textual features. The reason for focusing only on the extraction of textual features that is that ultimately, the work can be used to classify german fake news, for which only two datasets currently exist, which do not provide the information required for extracting news source features or environment features.\n",
    "\n",
    "So the goal of this work is to see how far you can get, using only the textual features and to compare the results based on the two mention german datasets as well as with the initially used dataset used in the work of Reis et al. with their results from the paper, as well as with the performance of a somewhat conventional classifier, which uses simple count vectors in regards to the features extraction.\n",
    "\n",
    "For the conventional classifier, a german fake news classifier from Dominik Leuziger is being used. (https://dagshub.com/leudom/german-fake-news-classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "#### Step 1 - Gathering data\n",
    "\n",
    "#### Step 1.1 - German data\n",
    "\n",
    "As mentioned, there are only two german dataset. Those are \n",
    "\n",
    "- GermanFakeNC: German Fake News Corpus (https://zenodo.org/record/3375714#.Ya8IHtDMKUk)\n",
    "- Kaggle Starter: Fake News Dataset German 9cc110a2-9 (https://www.kaggle.com/kerneler/starter-fake-news-dataset-german-9cc110a2-9/data )\n",
    "\n",
    "#### Step 1.1.1 - Scraping of GermanFakeNC articles\n",
    "\n",
    "- Scraping and generation of the GermanFakeNC dataset\n",
    "\n",
    "#### Step 1.1.2 - Preparation and merge of datasets\n",
    "\n",
    "- preparing and aligning the two german datasets\n",
    "- merging of datasets\n",
    "\n",
    "#### Step 1.2 - English data\n",
    "\n",
    "Optimally, we want to use the original dataset which was used in the work of Reis et al., \"Supervised Learning for Fake News Detection\". That is:\n",
    "\n",
    "- BuzzFace: A News Veracity Dataset with Facebook User Commentary and Ego (https://metatext.io/datasets/buzzface)\n",
    "\n",
    "As we will see in the progress of this work, acquiring that dataset in the shape that it was originally used is not possible. Hence another dataset is being used which is, not only with respect to size, not as extensive, but very close to the BuzzFace dataset in nature:\n",
    "\n",
    "- BuzzFeed-Webis Fake News Corpus 16 (https://webis.de/data/buzzfeed-webis-fake-news-16.html)\n",
    "\n",
    "#### Step 1.2.1 - Attempts of generating the BuzzFace dataset\n",
    "\n",
    "- showcase of failed attempt\n",
    "\n",
    "#### Step 1.2.2 - Generating the BuzzFeed-Webis dataset\n",
    "\n",
    "- Construction of the BuzzFeed-Webis dataset from XML files\n",
    "\n",
    "#### Step 2 - Performing the extensive (textual only) feature extration as proposed by Reis et al.\n",
    "\n",
    "- Step-by-Step reconstruction of 141 textual features\n",
    "\n",
    "#### Step 3 - Running the data (english and german) on the two best classifiers (XGBoost and RFs) used in Reis et al.'s work\n",
    "\n",
    "- Using XGBoost and RFs to compare the performance of the merged german dataset to the english (BuzzFeed-Webis) dataset\n",
    "\n",
    "#### Step 3.1 - Using stop words, stemming and  count vectorization\n",
    "\n",
    "\n",
    "#### Step 3.2 - Using the extensive feature extraction as proposed by Reis et al.\n",
    "\n",
    "\n",
    "#### Step 4 - Comparison of the feature extration methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Gathering data\n",
    "\n",
    "### Step 1.1 - German data\n",
    "\n",
    "### Step 1.1.1 - Scraping of GermanFakeNC articles\n",
    "\n",
    "This uses the GermanFakeNC and crawls through its entries to receive title and body from each samples URL\n",
    "\n",
    "Credit goes to https://dagshub.com/leudom/german-fake-news-classifier/src/master/src/data/scrape_news.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d84651c8b1c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mlog_fmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%(asctime)s - %(name)s - %(levelname)s - %(message)s'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_fmt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "import py7zr\n",
    "\n",
    "def extract_title(url):\n",
    "    article = Article(url)\n",
    "    try:\n",
    "        article.download()\n",
    "        logger.info('Article title downloaded from %s' % url)\n",
    "        article.parse()\n",
    "    except:\n",
    "        article.title = 'No title'\n",
    "\n",
    "    return article.title\n",
    "\n",
    "def extract_text(url):\n",
    "    article = Article(url)\n",
    "    try:\n",
    "        article.download()\n",
    "        logger.info('Article text downloaded from %s' % url)\n",
    "        article.parse()\n",
    "    except:\n",
    "        article.text = 'No text'\n",
    "\n",
    "    return article.text\n",
    "\n",
    "\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Load .env file\n",
    "# load_dotenv(find_dotenv())\n",
    "\n",
    "# INPUTFILE = os.path.join(os.getenv('PROJECT_DIR'),\n",
    "#                          'data',\n",
    "#                          'raw',\n",
    "#                          'GermanFakeNC.json')\n",
    "\n",
    "df_scraped_file = 'df_GermanFakeNC.csv'\n",
    "\n",
    "with py7zr.SevenZipFile('german_datasets.zip', 'r') as z:\n",
    "    for filename in z.namelist():\n",
    "        if filename == \"GermanFakeNC.json\":\n",
    "            df = pd.read_json(\"GermanFakeNC.json\")\n",
    "\n",
    "# df = pd.read_json(\"germanfakenc.json\")\n",
    "logger.info('Head of dataframe: \\n%s' % df.head())\n",
    "\n",
    "# %% We only take News with an overall rating of at least 0.5\n",
    "#overall_rating_mask = df['Overall_Rating'] >= 0.5\n",
    "##ratio_mask = df['Ratio_of_Fake_Statements'].isin([3, 4])\n",
    "#df_fake = df[overall_rating_mask & ratio_mask].reset_index()\n",
    "\n",
    "df['titel'] = df['URL'].apply(extract_title)\n",
    "df['text'] = df['URL'].apply(extract_text)\n",
    "\n",
    "logger.info('Head of dataframe after parsing: \\n%s' % df.head())\n",
    "\n",
    "# Filter rows with no information (titel or text)\n",
    "no_info_mask = (df['titel'] != 'No title') & (df['text'] != 'No text')\n",
    "df_final = df[no_info_mask]\n",
    "\n",
    "logger.info('Shape of final dataframe: %s' % str(df_final.shape))\n",
    "logger.info('dtypes: \\n%s' % str(df_final.dtypes))\n",
    "logger.info('Rows with null values: \\n%s' % df_final.isnull().sum())\n",
    "\n",
    "# Save as csv\n",
    "try:\n",
    "    df_final.to_csv(df_scraped_file, index=False)\n",
    "    logger.info(\"CSV was saved to disk\")\n",
    "except Exception:\n",
    "    logger.exception(\"Couldn't save CSV to disc \\n\", exc_info=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1.2 - Preparation and merge of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "\n",
    "def prepare_news_csv(filepath):\n",
    "    \"\"\" \n",
    "    1.) Drop columns -> Kategorie, Quelle, Art\n",
    "    2.) Check on duplicate Titel and Body and drop the first entry of duplicates\n",
    "    3.) Rename Columns in order to match it with the other dataset (GermanFakeNC)\n",
    "    4.) Add column source_name with news_csv to identifiy the source of a row after merging\n",
    "    \"\"\"\n",
    "\n",
    "    # Read news.csv from disk\n",
    "    _df = pd.read_csv(filepath)\n",
    "    logger.debug(_df.info())\n",
    "    # Drop cols\n",
    "    logger.info('Null values in news.csv: \\n%s' % _df.isnull().sum())\n",
    "    cols_to_drop = ['Kategorie', 'Quelle', 'Art']\n",
    "    _df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    logger.info('Cols %s dropped' % cols_to_drop)\n",
    "\n",
    "\n",
    "    # Drop duplicates\n",
    "    logger.info('Percent duplicated Titel and Body: \\n%s' % str(\n",
    "        _df.duplicated(subset=['Titel', 'Body']).value_counts(normalize=True)))\n",
    "    _df.drop_duplicates(subset=['Titel', 'Body'], inplace=True)\n",
    "    logger.info('Duplicates in Titel and Body dropped')\n",
    "\n",
    "    # Rename Cols\n",
    "    new_cols = {'id': 'src_id',\n",
    "                'Titel': 'title',\n",
    "                'Body': 'text',\n",
    "                'Datum': 'date',\n",
    "                'Fake': 'fake'}\n",
    "    _df.rename(columns=new_cols, inplace=True)\n",
    "    logger.info('Cols renamed')\n",
    "\n",
    "    # Add col source_name\n",
    "    _df['src_name'] = 'news_csv'\n",
    "\n",
    "    return _df\n",
    "\n",
    "\n",
    "def prepare_germanfake(filepath):\n",
    "    \"\"\" \n",
    "    1.) Drop columns -> [False_Statement_1_Location,\n",
    "                         False_Statement_1_Index,\n",
    "                         False_Statement_2_Location,\n",
    "                         False_Statement_2_Index,\n",
    "                         False_Statement_3_Location,\n",
    "                         False_Statement_3_Index,\n",
    "                         Ratio_of_Fake_Statements,\n",
    "                         Overall_Rating]\n",
    "        We treat all entries as fakenews, eventhough there are some instances\n",
    "        that have a very low fake overall ratings!!\n",
    "    2.) Make index source_id\n",
    "    3.) Check on duplicate titel and text and drop the first entry of duplicates\n",
    "    4.) Drop rows where titel or text is null \n",
    "    5.) Fill Dates for missing values -> From the URL we can see that the Date could\n",
    "        be 2017/12 \n",
    "    6.) Rename Columns in order to match it with the other dataset (news.csv)\n",
    "    7.) Add label col 'fake' = 1 -> all 1; col 'src_name' = 'GermanFakeNC'\n",
    "    \"\"\"\n",
    "\n",
    "    # Read news.csv from disk\n",
    "#     _df = pd.read_csv(filepath)\n",
    "    \n",
    "    logger.debug(_df.info())\n",
    "    # Drop cols\n",
    "    logger.info('Null values in GermanFakeNC_interim.csv: \\n%s' % _df.isnull().sum())\n",
    "    cols_to_drop = ['False_Statement_1_Location',\n",
    "                    'False_Statement_1_Index',\n",
    "                    'False_Statement_2_Location',\n",
    "                    'False_Statement_2_Index',\n",
    "                    'False_Statement_3_Location',\n",
    "                    'False_Statement_3_Index',\n",
    "                    'Ratio_of_Fake_Statements',\n",
    "                    'Overall_Rating']\n",
    "    _df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    logger.info('Cols %s dropped' % cols_to_drop)\n",
    "\n",
    "    # Set source_id\n",
    "    _df.reset_index(inplace=True)\n",
    "    logger.info('Index reset')\n",
    "    \n",
    "    # Drop duplicates\n",
    "    logger.info('Percent duplicated titel and text: \\n%s' % str(\n",
    "        _df.duplicated(subset=['titel', 'text']).value_counts(normalize=True)))\n",
    "    _df.drop_duplicates(subset=['titel', 'text'], inplace=True)\n",
    "    logger.info('Duplicates in titel and text dropped')\n",
    "\n",
    "    # Drop rows where titel or text is null\n",
    "    _df.dropna(subset=['titel', 'text'], inplace=True)\n",
    "    logger.info('Null rows for titel and text dropped')\n",
    "\n",
    "    # Fill the missing dates\n",
    "    _df['Date'].fillna(pd.to_datetime('01/12/2017'), inplace=True)\n",
    "\n",
    "    # Rename Cols\n",
    "    new_cols = {'index': 'src_id',\n",
    "                'titel': 'title',\n",
    "                'Date': 'date',\n",
    "                'URL': 'url'}\n",
    "    _df.rename(columns=new_cols, inplace=True)\n",
    "    logger.info('Cols renamed')\n",
    "\n",
    "    # Add col source_name\n",
    "    _df['fake'] = 1\n",
    "    _df['src_name'] = 'GermanFakeNC'\n",
    "\n",
    "    return _df\n",
    "\n",
    "\n",
    "def merge_datasets(df_1, df_2):\n",
    "    logger.info('Shape: %s\\n Columns: %s' % (df_1.shape, df_1.columns))\n",
    "    logger.info('Shape: %s\\n Columns: %s' % (df_2.shape, df_2.columns))\n",
    "    # Check col names\n",
    "    sym_diff = set(df_1).symmetric_difference(set(df_2))\n",
    "    assert len(sym_diff) == 0 , 'Differences in colnames of the two datasets'\n",
    "    return pd.concat([df_1, df_2], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s : %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# find .env automagically by walking up directories until it's found, then\n",
    "# load up the .env entries as environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "NEWS_CSV = os.path.join('news.csv')\n",
    "GERMAN_FAKE_NC = os.path.join('df_GermanFakeNC.csv')\n",
    "OUTPUT = os.path.join('datasets_merged.csv')\n",
    "\n",
    "df_news = prepare_news_csv(NEWS_CSV)\n",
    "df_gfn = prepare_germanfake(GERMAN_FAKE_NC)\n",
    "df_merged = merge_datasets(df_news, df_gfn)\n",
    "\n",
    "try:\n",
    "    df_merged.to_csv(OUTPUT, sep=';', index=False)\n",
    "    logger.info('Final dataset prepared and saved to %s' % OUTPUT)\n",
    "except Exception:\n",
    "    logger.exception('File could not be daved to disk\\n', exc_info=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2 - English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2.1 - Attempts of generating the BuzzFace dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2.2 - Generating the BuzzFeed-Webis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Performing the extensive (textual only) feature extration of Reis et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Running the data on the two best classifiers (XGBoost and RFs) used in Reis et al.'s work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 - Using stop words, stemming and  count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 - Using the extensive feature extraction as proposed by Reis et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Comparison of the feature extration methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "#### Performing the extensive (textual only) feature extration as proposed by Reis et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
